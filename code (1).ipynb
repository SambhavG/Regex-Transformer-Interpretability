{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsOoab2nQHSw",
        "outputId": "6967a2d7-8f6e-4e69-fd40-d5cc6d016a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "966\n",
            "966\n",
            "991 991\n",
            "1544\n",
            "388\n",
            "1982\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "def generate_valid(length):\n",
        "    num_a = random.randint(0, length)\n",
        "    num_b = length - num_a\n",
        "    return \"a\" * num_a + \"b\" * num_b\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "      while True:\n",
        "          # Random string of a's and b's which isn't a valid a*b* string\n",
        "          invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "          if \"ba\" in invalid_str:\n",
        "              break\n",
        "      num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "      return (\n",
        "          START_TOKEN\n",
        "          + PADDING_TOKEN * num_p\n",
        "          + invalid_str\n",
        "          + END_TOKEN\n",
        "          + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "      )\n",
        "    else:\n",
        "      string = generate_valid(length)\n",
        "      index = random.randint(0, length - 2)\n",
        "      string[index] = 'b'\n",
        "      string[index + 1] = 'a'\n",
        "      num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "      return (\n",
        "          START_TOKEN\n",
        "          + PADDING_TOKEN * num_p\n",
        "          + invalid_str\n",
        "          + END_TOKEN\n",
        "          + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "      )\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 1000  # Total number of samples\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "print(len(valid_dataset))\n",
        "print(len(invalid_dataset))\n",
        "\n",
        "split = len(valid_dataset) * 4 // 5;\n",
        "train_dataset = valid_dataset[:split] + invalid_dataset[:split]\n",
        "test_dataset = valid_dataset[split:] + invalid_dataset[split:]\n",
        "\n",
        "num_ood_samples = 1000\n",
        "dataset = []\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "ood_valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "ood_invalid_dataset = list(set(dataset))[:len(ood_valid_dataset)]\n",
        "print(len(ood_valid_dataset), len(ood_invalid_dataset))\n",
        "\n",
        "ood_dataset = ood_valid_dataset + ood_invalid_dataset\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(ood_dataset))\n",
        "\n",
        "\n",
        "# Write to file\n",
        "with open(\"train_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in train_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"test_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in test_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"ood_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in ood_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 1\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class ActivationDatasetGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(ActivationDatasetGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "state_dict = torch.load('/content/1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth')\n",
        "\n",
        "datagen_model = ActivationDatasetGenerator(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "del state_dict['fc.weight']\n",
        "del state_dict['fc.bias']\n",
        "\n",
        "datagen_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2-QYYG-icxx",
        "outputId": "5691cd30-17c6-4762-e0c7-8e90cce3dfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "    while True:\n",
        "        # Random string of a's and b's which isn't a valid a*b* string\n",
        "        invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "        if \"ba\" in invalid_str:\n",
        "            break\n",
        "    num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + invalid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "    )\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 1000  # Total number of samples\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append(x)\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append(x)\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "\n",
        "dataset = invalid_dataset\n",
        "\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "def encode_string(string):\n",
        "    return [char_to_index[char] for char in string]\n",
        "\n",
        "class ActivationDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "    def add_data(self, string, tensor):\n",
        "        for i in range(len(string)):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            elif string[i - 1] == 'a' and string[i] == 'a':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([1]).long())\n",
        "            elif string[i - 1] == 'b' and string[i] == 'b':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([2]).long())\n",
        "            elif string[i - 1] == 'a' and string[i] == 'b':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([3]).long())\n",
        "            elif string[i - 1] == 'b' and string[i] == 'a':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([4]).long())\n",
        "            else:\n",
        "                if i % 5 == 0:\n",
        "                    self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                    self.labels.append(torch.Tensor([0]).long())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return data, torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "activation_dataset = ActivationDataset()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(dataset)):\n",
        "      data_val = encode_string(dataset[i])\n",
        "      activation_val = datagen_model(torch.tensor(data_val, dtype=torch.long))\n",
        "      activation_dataset.add_data(dataset[i], activation_val)\n",
        "\n",
        "print(len(activation_dataset))\n",
        "print(activation_dataset.data[0])\n",
        "print(activation_dataset.labels[0])\n",
        "torch.save(activation_dataset, 'activations2_20000_split.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVIzRUXmkdnM",
        "outputId": "3a680fd4-1f0d-4605-cc1c-0e755cddb0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203522\n",
            "tensor([[-2.0460, -0.1504,  1.6408, -0.4820, -0.8400,  1.7815],\n",
            "        [-2.0039, -0.3235,  1.5289, -0.5143, -0.6699,  1.9068]])\n",
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureProbe(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureProbe, self).__init__()\n",
        "        # lol this is tiny\n",
        "        self.mlp = nn.Linear(12, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x.reshape(len(x),-1))\n",
        "        return x\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "model = FeatureProbe()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "dataloader = DataLoader(activation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device).long().squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = torch.zeros(5).to(device)\n",
        "    total = torch.zeros(5).to(device)\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.argmax(torch.round(outputs).squeeze(), axis=-1)\n",
        "        # print(labels.shape)\n",
        "        # print(predicted.shape)\n",
        "        # atrocious code\n",
        "        for i in range(5):\n",
        "            total[i] += torch.sum(labels.to(device) == i).item()\n",
        "            correct[i] += torch.sum((predicted.squeeze().to(device) == labels.squeeze().to(device)) * (labels.squeeze().to(device) == i)).item()\n",
        "\n",
        "    print(correct)\n",
        "    print(total)\n",
        "    for i in range(5):\n",
        "        print(i, correct[i] / total[i], total[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEE-tEryruK2",
        "outputId": "5b7697c8-8731-4159-d950-6276b9f68240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-50228ac1b55b>:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return data, torch.tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.38158777356147766\n",
            "tensor([44389., 39635., 39792., 36813., 36997.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9962, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1.0000, device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9995, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9260, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9305, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 2/5, Loss: 0.156574547290802\n",
            "tensor([44418., 39636., 39808., 39386., 39307.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9999, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9907, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9886, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 3/5, Loss: 0.1009211540222168\n",
            "tensor([44420., 39636., 39809., 39707., 39720.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9999, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9987, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9990, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 4/5, Loss: 0.06118711829185486\n",
            "tensor([44421., 39636., 39810., 39753., 39739.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(1.0000, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9999, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9994, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 5/5, Loss: 0.041719596832990646\n",
            "tensor([44421., 39636., 39811., 39756., 39753.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(1., device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(1.0000, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9998, device='cuda:0') tensor(39761., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um9d6gaFQHS2",
        "outputId": "6f8fb310-c53b-4460-f059-83a83611d30c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.0643521100282669\n",
            "Accuracy: 0.9852941176470589\n",
            "Epoch 2/5, Loss: 0.09316340833902359\n",
            "Accuracy: 0.9913097454996896\n",
            "Epoch 3/5, Loss: 0.0352286621928215\n",
            "Accuracy: 0.9910724066162778\n",
            "Epoch 4/5, Loss: 0.025056665763258934\n",
            "Accuracy: 0.9913827728484318\n",
            "Epoch 5/5, Loss: 0.04972020909190178\n",
            "Accuracy: 0.9928341914046811\n",
            "Epoch 1/5, Loss: 0.02033993974328041\n",
            "Accuracy: 0.9934731807061744\n",
            "Epoch 2/5, Loss: 0.028123032301664352\n",
            "Accuracy: 0.9935827217292876\n",
            "Epoch 3/5, Loss: 0.03850043565034866\n",
            "Accuracy: 0.9931719428926132\n",
            "Epoch 4/5, Loss: 0.022266890853643417\n",
            "Accuracy: 0.9941943257750028\n",
            "Epoch 5/5, Loss: 0.04888172447681427\n",
            "Accuracy: 0.9939296016358126\n",
            "Accuracy: 0.9940484883890756\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants for model\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 100\n",
        "OOD_MAX_LENGTH = 500\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = StringDataset(\"train_dataset_padded (1).txt\")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"1head_1layer_embed6revbatch512hidden1_100max500ood_total_transformer_model.pth\")\n",
        "\n",
        "test_dataset = StringDataset(\"test_dataset_padded (1).txt\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in test_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgtAxiKqojlD",
        "outputId": "ecb9b0c7-f8e8-4b59-dcc5-e95c00be44f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9719818488813845\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_padded (1).txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQF3oa6oQHS5",
        "outputId": "ef5f5146-b590-4bc3-f1a5-57de88f25be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 examples which are wrongly classified\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababbaaaaaaaaaabbbbaaaabaababaabaabbbbaabaabbbbabaaaabbaababbaaaabbabbbabaabbababbababbabbaaaaabbabaaabbaaaaaabaaabbbaabbaaabbaaaabaababaaaabbaaabbbbabaabbabaaaabbbababaaaaabbabaababbbaaaabbaaabbbabaaabbbbabbbaaaababbbabbbaababaabbbabaaabbaabbbbaaababbaaabaabbbbaaabbabaaaaaabaaabaabbbbbaaaaaabaababbbbbabaaabbbabbbabbbbbbbabbbaabbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaabbbbaababaabababbbbbbbbbbabaabbbabbbaabbaabbbaabbbabbabbabbbbaaabbababbaabbbabbaaabbabbbbabbbbabbaaabaabaaaabaabbababaaababababbbbaaababaaaaabaabbbaaabbabaababbababaabaaabbaaabbbbaabbbbbbbbababaaaabaabaabaaababbbabbbbabaabaabbbbaaaabaabaaaabbbbabaaaabbbabbbbbababbabbabaababbbabaaaabbbbaabbbaabaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbabaaaaabaababbabbaabbabbbbaabbabbaaabbababbbbbbbabaabbbaaaaaababaaaaaaaabbabbbababbaabbabaabbaaaaabaabbabbaaaaaaabbabbabbbaaabbabbbbabbabbbbaabbbbaaaaaabaaaaaabbbabaaaabaaaababaaabepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaabbaaaaabbabbbbaababaaaababbabbbaaaaabbababbababaaabbaababbbbabbaaabaabbbaabbbbbaaabababbabbbaaabbbabbaabbaaaabbabbabaabbbabbbbbabaaaaaabbaabbaababaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbabaaabbaaababaabbaaabaaabbaababbabbbbbaabaabbbbaabbaabaabbbbbabaaaabbabaabaabaabbaaabababbbabaaabaaaababbbabaabaaaaabaaaaaaabbaaaaaaabababbbbbabaabbabbbababbbbbbaaabaaaaabaaababbaaaaaababababaabbabbbbbbbabbbababbbaabbbbbabbaabbbbbabbbbbaabababbbbbbbbaabaabbaaaabbbabaabaabaabbbbbaabbbbbbbabbbabababbaaaababbbbbabababaabbbaaaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaabbabaababaaaababbbabbbbaaabbbbaaababababbababbbaabbbabaabbababbabbbbbababbababbabbbaabbabbaabbbbabbbaaabbbbabbabaabbaababaabaabaabbabaaaabbabbbaaaabaaaabaaabaabaaaaababbbabaaababbabababbbbaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababaaabbaabaabbbaaababbaaaabbbbaabbbbaaababbaabaabaaaabaabbaaababaabbbbabbaababbaabaababaaabbabbbbbaaabaaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaabaabaaabbbbabbbabaaabababbaaaaaababbabaabbabbbbbbbababababaaabaababaabbaaaabaabbabbbabbbbabbbaaaaabababbbbabbabbbaaabbaabeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaababbaaabbbabbbaabaaababaaabaabbaaababbaaaabbaabbabbabbaabbabaabaaaaaababbababbbababbabbabbbaaaaabbbbbbababaaababbbabbaaaabbbbabbabaaaaaabbbbabbbaaabbaaaabbaaabaabbabbbabaaaaabbbabbababaaabbaaaabbbabaabaaaaabbaababbbbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaababbaaaaabaaaababbababbbbababaabaabaabbbbbbaaaaabbaabbabaababbaabbbbbbbbbabbbaabaaabbabaabbabbaababbbaabbbbaababbbaaabbbbbaaabbbbbaaaabbbbbbbaaabbbabbbaabbaaabbbababbbaababababaaabbabaaabaabbaababaaabbaabababaabababaaaabaabbbabaaaabaaabbaaaabababbaaabbaabaaaababababaabababbaaababaaabaabbababbabbbbbbbbbaaaaabaabbbabbbaaaabbabbbbbabaabbbababaabaabababbbabbaaaaaabaabbbabaaabbaabbaabaababbbabaaaabaabaababbbbababbbbabaaabaaabaababbbabbaabbbabaaaabbaaababbbabbaabaababbabababbbabaabbaabbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbaabaabbabbbbaababbbaaaaababbbaababbaabbaababaababaaabababbaaabababababbbbbbbaaababbaaabaabbaabaaabaabbbbaababbbbaaaaaaaabaabbaaabbabaababbbabababaabbbaabbaaabbabaaabbababaaaabbbaabaabbbbabbbbaaabbbbaabbababbbababbababababaabaaaaaaaabbabaababababbbbababaababaaaabaaaabababbbabbbabbbaaabababbabbabbabaaaabbbabaaababbbaaaaaabaaabaaaaababababbabaababbbaaaabaaaaabbabaababaaaaababaabaaabaabababaababbaababbabbbaaabbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaaabbbabbabbbbbbaaabaaaaabbabaaabaabbaaaabaabbbaabaabbbbbbbbabaabbbbbaabbbaabbaabbaabbbbababaaabbaaabbbbbaabbaabbababaaabbaaabbaaaabbaababbaaaaababaabaaababaabbbbbaabaabbabbbbababbbbbaababbaababbbbbaabbababbaaaaaabbbababaabaaaaaaabbaabababaaabbaababbbbbbaabbabbbaabababbbbbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n"
          ]
        }
      ],
      "source": [
        "# Print 20 examples which are wrongly classified\n",
        "print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        if count == 20:\n",
        "            break\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            string = \"\".join([VALID_CHARACTERS[int(idx)] for idx in inputs[i]])\n",
        "            print(string, labels[i].item(), predicted[i].item())\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0WOiMFYFLKF",
        "outputId": "cb6efb8a-2ed4-403a-84f6-7b829b029a7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5.4745e+04 6.6200e+02]\n",
            " [2.9000e+01 5.4112e+04]]\n",
            "[691.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "model_2 = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_2.load_state_dict(torch.load('1head_1layer_embed6revbatch512hidden1_100max500ood_total_transformer_model.pth'))\n",
        "model_2.to(device)\n",
        "\n",
        "# Print 20 examples which are wrongly classified\n",
        "# print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "matrix = np.zeros((2, 2))\n",
        "num_pad_zeros = np.zeros((500))\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        matrix[int(predicted[i]), int(labels[i])] += 1\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            num_start_pad = 0\n",
        "            for j in range(len(inputs[i])):\n",
        "                if VALID_CHARACTERS[int(inputs[i][j])] != 'p':\n",
        "                    break\n",
        "                num_start_pad += 1\n",
        "            num_pad_zeros[num_start_pad] += 1\n",
        "print(matrix)\n",
        "print(num_pad_zeros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ekqsmo6JWde",
        "outputId": "6057ec8a-00aa-4f05-fe39-bcdef45b8e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9748629848229342\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_gappadded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model_2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtNf76gYGIIe",
        "outputId": "3ae629bc-80b3-40f3-c7f7-6ddc0f6f81f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1173, -0.4868, -0.2787, -0.3836,  0.4869,  0.4462],\n",
            "        [-0.3077,  0.1080, -0.0703,  0.2785,  0.0020,  0.3271],\n",
            "        [-0.3017, -0.1836, -0.0864, -0.2321, -0.2330, -0.4434],\n",
            "        [-0.0861,  0.6321, -0.4590, -0.3281, -0.5029, -0.4216],\n",
            "        [ 0.4099, -0.1211, -0.1174, -0.1267, -0.3700, -0.1985],\n",
            "        [ 0.3000,  0.4385, -0.3269,  0.3167,  0.4170, -0.3065],\n",
            "        [ 0.1610, -0.0720,  0.3529,  0.4052,  0.0425,  0.2735],\n",
            "        [-0.3191, -0.0640,  0.2417, -0.0950,  0.0050, -0.3506],\n",
            "        [-0.3651, -0.1423, -0.1854, -0.2698,  0.1397,  0.0741],\n",
            "        [ 0.4333,  0.2350, -0.0254,  0.4441,  0.2188,  0.1552],\n",
            "        [ 0.1111, -0.1373, -0.0707,  0.4105,  0.1355,  0.3309],\n",
            "        [-0.2359, -0.2420, -0.0904,  0.2508,  0.3944, -0.0200],\n",
            "        [-0.2139,  0.3799, -0.1082, -0.3886,  0.0216, -0.0505],\n",
            "        [-0.2125,  0.3423, -0.3185,  0.3012, -0.3671,  0.2281],\n",
            "        [-0.4219,  0.1814,  0.2821,  0.0127,  0.1133,  0.3603],\n",
            "        [-0.2622, -0.2055,  0.4117,  0.4700,  0.1935,  0.2158],\n",
            "        [ 0.0299, -0.0371, -0.0226,  0.0525,  0.0328, -0.4088],\n",
            "        [-0.1115,  0.3936, -0.3476,  0.4100, -0.1752,  0.3781]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(model_2.transformer_encoder.layers[0].self_attn.in_proj_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2F50v9OH4EL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Transformer model\n",
        "class TransformerDebugClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerDebugClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layer = TransformerEncoderLayerWithAttention(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoderWithAttention(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        print(x.shape)\n",
        "        x, attn = self.transformer_encoder(x)\n",
        "        print(x.shape)\n",
        "        print(attn)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvrxJ8MMIZOV",
        "outputId": "23b4fc1b-c5f9-4386-db0a-d687f3aaac48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9727547931382442\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model_3 = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_3.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "ood_dataset = StringDataset(\"ood_dataset_padded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model_3.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "device = 'cpu'\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_3(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1v80IUlUNPFH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5616325-c16a-46fd-ea13-f331936d7b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Policy Gradient = -0.30830949544906616\n",
            "tensor([ 0.0100, -0.0100,  0.0100,  0.0100,  0.0100,  0.0100,  0.0100, -0.0100,\n",
            "         0.0100,  0.0100, -0.0100, -0.0100,  0.0100,  0.0100,  0.0100, -0.0100,\n",
            "        -0.0100,  0.0100,  0.0100, -0.0100], requires_grad=True)\n",
            "tensor(0.0044, grad_fn=<MeanBackward0>)\n",
            "Episode 100: Policy Gradient = -0.8414623141288757\n",
            "tensor([ 0.2731,  0.3374,  0.4244,  0.4008,  0.3439,  0.1844,  0.3444,  0.2955,\n",
            "         0.1875,  0.1238,  0.0893,  0.1798,  0.0324,  0.0545,  0.0743, -0.0716,\n",
            "        -0.0245,  0.0544,  0.1404,  0.0916], requires_grad=True)\n",
            "tensor(0.0115, grad_fn=<MeanBackward0>)\n",
            "Episode 200: Policy Gradient = -0.12202877551317215\n",
            "tensor([0.4214, 0.4827, 0.6143, 0.6338, 0.4996, 0.3066, 0.5284, 0.3704, 0.3276,\n",
            "        0.2400, 0.1361, 0.3238, 0.1117, 0.0597, 0.1474, 0.0357, 0.0519, 0.1241,\n",
            "        0.1061, 0.0736], requires_grad=True)\n",
            "tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Episode 300: Policy Gradient = -0.01457495428621769\n",
            "tensor([0.5413, 0.6340, 0.7342, 0.8018, 0.6034, 0.4394, 0.6349, 0.4560, 0.4354,\n",
            "        0.3530, 0.2124, 0.3283, 0.1126, 0.1302, 0.1486, 0.0730, 0.0695, 0.1410,\n",
            "        0.0725, 0.1524], requires_grad=True)\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>)\n",
            "Episode 400: Policy Gradient = -0.015081866644322872\n",
            "tensor([0.6700, 0.7884, 0.8763, 0.9900, 0.7389, 0.6048, 0.7766, 0.6222, 0.5813,\n",
            "        0.4491, 0.2262, 0.4247, 0.2408, 0.1915, 0.0724, 0.0498, 0.1684, 0.1337,\n",
            "        0.0464, 0.1333], requires_grad=True)\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>)\n",
            "Episode 500: Policy Gradient = -0.11036746948957443\n",
            "tensor([0.7346, 0.8572, 0.9665, 1.0641, 0.8085, 0.6703, 0.8238, 0.6845, 0.5986,\n",
            "        0.4471, 0.2866, 0.4379, 0.2330, 0.1705, 0.0434, 0.0688, 0.1740, 0.1455,\n",
            "        0.0642, 0.1081], requires_grad=True)\n",
            "tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "Episode 600: Policy Gradient = -0.011846587061882019\n",
            "tensor([0.8206, 0.9385, 1.0432, 1.1360, 0.8750, 0.7211, 0.8793, 0.7040, 0.6237,\n",
            "        0.4948, 0.3495, 0.4774, 0.2639, 0.2124, 0.0338, 0.0827, 0.1579, 0.1713,\n",
            "        0.1201, 0.1269], requires_grad=True)\n",
            "tensor(0.0002, grad_fn=<MeanBackward0>)\n",
            "Episode 700: Policy Gradient = -0.13120804727077484\n",
            "tensor([0.8622, 0.9981, 1.0957, 1.1920, 0.9235, 0.7466, 0.9250, 0.7389, 0.6371,\n",
            "        0.5255, 0.3801, 0.4831, 0.2623, 0.2199, 0.0450, 0.1093, 0.1775, 0.1678,\n",
            "        0.1124, 0.1327], requires_grad=True)\n",
            "tensor(0.0017, grad_fn=<MeanBackward0>)\n",
            "Episode 800: Policy Gradient = -0.042562708258628845\n",
            "tensor([0.9150, 1.0613, 1.1506, 1.2488, 1.0078, 0.8218, 0.9686, 0.7873, 0.6921,\n",
            "        0.5739, 0.3815, 0.4887, 0.2922, 0.2292, 0.0765, 0.1231, 0.1830, 0.1698,\n",
            "        0.1278, 0.1420], requires_grad=True)\n",
            "tensor(0.0006, grad_fn=<MeanBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-71a3ce8f2a70>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mmake_fooling_character\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-71a3ce8f2a70>\u001b[0m in \u001b[0;36mmake_fooling_character\u001b[0;34m(X, model, string_length)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Generate an episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOOD_MAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-71a3ce8f2a70>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mlog_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcurr_result\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mextract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    377\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \"\"\"\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# First call the original checkcache as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# to our compiled codes can be produced.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mcontinue\u001b[0m   \u001b[0;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mstat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def is_valid(string):\n",
        "  return (\"ba\" not in string)\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "class AdversaryNet(nn.Module):\n",
        "    def __init__(self, x):\n",
        "        super(AdversaryNet, self).__init__()\n",
        "        self.logits = x\n",
        "\n",
        "    def forward(self):\n",
        "        # sigmoid to sum to 1, batchsize\n",
        "        probs = torch.sigmoid(self.logits)\n",
        "        result = torch.zeros(5,len(self.logits)).long()\n",
        "        log_prob = torch.zeros(5)\n",
        "        for i in range(5):\n",
        "            curr_result = torch.bernoulli(probs).long()\n",
        "            log_prob[i] = torch.sum(torch.log(probs * curr_result + (1 - probs) * (1 - curr_result)))\n",
        "            result[i,:] = curr_result\n",
        "        return result, log_prob\n",
        "\n",
        "def make_fooling_character(X, model, string_length=20):\n",
        "    \"\"\"\n",
        "    Generate a fooling distribution: an invalid string that the model classifies\n",
        "    as valid. In this case, start with random noise and perform gradient ascent.\n",
        "\n",
        "    Input: Tensor of shape (string_length) between [0, 1], sample a and b from that\n",
        "    Model: Pretrained predictor model\n",
        "\n",
        "    Returns: Fooling distribution\n",
        "    \"\"\"\n",
        "\n",
        "    net = AdversaryNet(X)\n",
        "    optimizer = optim.Adam([net.logits], lr=0.01)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        # Generate an episode\n",
        "        actions, log_prob = net()\n",
        "\n",
        "        string = torch.zeros(5, OOD_MAX_LENGTH).long()\n",
        "        string[:,0] = 0\n",
        "        string[:,1:string_length + 1] = actions + 1\n",
        "        string[:,string_length + 1] = 3\n",
        "        string[:,string_length + 2:] = 4\n",
        "\n",
        "        # Simulate the environment and get rewards\n",
        "        rewards = model.forward(string)  # Reward based on sampled actions\n",
        "        # print(rewards)\n",
        "\n",
        "        # Calculate the policy gradient\n",
        "        policy_gradient = torch.zeros_like(log_prob)\n",
        "        for i in range(5):\n",
        "          policy_gradient[i] = (rewards[i][0]) * log_prob[i]\n",
        "        policy_gradient = policy_gradient.sum()\n",
        "\n",
        "        # Update the policy network\n",
        "        optimizer.zero_grad()\n",
        "        policy_gradient.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode}: Policy Gradient = {policy_gradient.item()}')\n",
        "            print(net.logits)\n",
        "            print(rewards.mean())\n",
        "\n",
        "make_fooling_character(torch.zeros(20).requires_grad_(), model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "def is_valid(string):\n",
        "  return (\"ba\" not in string)\n",
        "\n",
        "def generate_strings(length):\n",
        "    # Define the characters\n",
        "    characters = ['a', 'b']\n",
        "\n",
        "    strings = []\n",
        "    # Generate all combinations of characters of given length\n",
        "    for i in range(length):\n",
        "      strings.append(\"a\" * i + \"b\" * (length - i))\n",
        "\n",
        "    return strings\n",
        "\n",
        "# Generate all strings of length 8\n",
        "strings_length_8 = generate_strings(20)\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "for string in strings_length_8:\n",
        "    str2 = \"s\" + \"p\"*5 + string + \"e\" + \"p\"*(400 - 27)\n",
        "    if (model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)) > 0.7 and is_valid(str2)):\n",
        "      print(str2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF-Inn-j7cRm",
        "outputId": "a85b9095-8c16-4b5d-a839-1fbd6fe4d746"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spppppaaaaaaaabbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaabbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaabbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaabbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaabbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaabbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaabbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaaabbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaaaabbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaaaaabbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaaaaaabbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "spppppaaaaaaaaaaaaaaaaaaabeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"spaaaabaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "\n",
        "class TransformerClassifier2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def return_pre_fc(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        sum = self.fc.weight.data.squeeze() * x\n",
        "        return sum\n",
        "\n",
        "model = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "vals = model.return_pre_fc(torch.tensor([char_to_index[char] for char in text], dtype=torch.long))\n",
        "vals = vals.view(400, 6).sum(axis=-1)\n",
        "print(vals[0:12])\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in text], dtype=torch.long)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJozdr3VE5SP",
        "outputId": "ec514f1f-2e9c-4289-9125-9cca6a7dcda9"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0241,  1.6128, -0.1938,  0.0503,  0.2070, -0.1356, -0.8409,  0.0285,\n",
            "         0.2954,  0.3082,  0.4625,  0.4214], grad_fn=<SliceBackward0>)\n",
            "tensor([[0.5452]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as mcolors\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def format(value):\n",
        "  c, l = value\n",
        "  return \"<span style='color:{};'>{}</span>\".format(l,c)\n",
        "\n",
        "def format_chars(chars,numbers):\n",
        "    print(chars)\n",
        "    print(numbers)\n",
        "    numbers = np.array(numbers.detach()).astype(float)\n",
        "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
        "    cmap = cm.RdYlGn\n",
        "    colors = cmap(norm(numbers))\n",
        "    hexcolor = [mcolors.to_hex(c) for c in colors]\n",
        "    text = \" \".join(list(map(format, zip(chars,hexcolor))))\n",
        "    text = \"<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'>\" + text + \"</div>\"\n",
        "    display(HTML(text))\n",
        "    return colors\n",
        "\n",
        "format_chars(list(text[0:20]), vals[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "5fHKpV-gHmuV",
        "outputId": "48939fca-c72e-45a4-a2d7-723745ff2bdd"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'p', 'a', 'a', 'a', 'a', 'b', 'a', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([ 0.0241,  1.6128, -0.1938,  0.0503,  0.2070, -0.1356, -0.8409,  0.0285,\n",
            "         0.2954,  0.3082,  0.4625,  0.4214,  0.3329,  0.2554,  0.2074,  0.1694,\n",
            "         0.2186,  0.1501,  0.1455,  0.2175], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fafdb8;'>s</span> <span style='color:#006837;'>p</span> <span style='color:#fee18d;'>a</span> <span style='color:#f5fbb2;'>a</span> <span style='color:#d7ee8a;'>a</span> <span style='color:#feea9b;'>a</span> <span style='color:#cc2627;'>b</span> <span style='color:#fafdb8;'>a</span> <span style='color:#c1e57b;'>e</span> <span style='color:#bde379;'>p</span> <span style='color:#91d068;'>p</span> <span style='color:#a0d669;'>p</span> <span style='color:#b7e075;'>p</span> <span style='color:#cbe982;'>p</span> <span style='color:#d7ee8a;'>p</span> <span style='color:#dff293;'>p</span> <span style='color:#d5ed88;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#e3f399;'>p</span> <span style='color:#d5ed88;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97954633, 0.99138793, 0.72103037, 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.99623222, 0.88319877, 0.55309496, 1.        ],\n",
              "       [0.96201461, 0.98400615, 0.6970396 , 1.        ],\n",
              "       [0.84313725, 0.93387159, 0.54002307, 1.        ],\n",
              "       [0.99730873, 0.91657055, 0.60907343, 1.        ],\n",
              "       [0.80084583, 0.14763552, 0.15209535, 1.        ],\n",
              "       [0.97954633, 0.99138793, 0.72103037, 1.        ],\n",
              "       [0.75686275, 0.89665513, 0.48419839, 1.        ],\n",
              "       [0.74117647, 0.8898885 , 0.47404844, 1.        ],\n",
              "       [0.56732026, 0.81437908, 0.40653595, 1.        ],\n",
              "       [0.62637447, 0.8402153 , 0.412995  , 1.        ],\n",
              "       [0.71764706, 0.87973856, 0.45882353, 1.        ],\n",
              "       [0.79607843, 0.9135717 , 0.50957324, 1.        ],\n",
              "       [0.84313725, 0.93387159, 0.54002307, 1.        ],\n",
              "       [0.87435602, 0.94709727, 0.57708574, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.89188774, 0.95447905, 0.60107651, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"spaabbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "text3 = \"spaaaaaaabbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "model2 = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model2.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "vals2 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text2], dtype=torch.long))\n",
        "vals3 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text3], dtype=torch.long))\n",
        "vals2 = vals2.view(400, 6).sum(axis=-1)\n",
        "vals3 = vals3.view(400, 6).sum(axis=-1)\n",
        "\n",
        "format_chars(list(text2[0:20]), vals2[0:20])\n",
        "format_chars(list(text3[0:20]), vals3[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "dD1CXQPWM-Cx",
        "outputId": "64c2f002-e955-40ae-9113-6115629b1367"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'p', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([ 0.0162,  1.4855,  0.2543,  0.0503, -1.1846, -1.0383, -0.8250, -0.7753,\n",
            "        -0.4368, -0.5359, -0.4594, -0.3800,  0.1013,  0.2502,  0.2274,  0.1753,\n",
            "         0.2210,  0.0988,  0.1550,  0.2178], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fbfdba;'>s</span> <span style='color:#006837;'>p</span> <span style='color:#cbe982;'>a</span> <span style='color:#f5fbb2;'>a</span> <span style='color:#a50026;'>b</span> <span style='color:#a50026;'>b</span> <span style='color:#d02927;'>b</span> <span style='color:#da362a;'>b</span> <span style='color:#fba35c;'>b</span> <span style='color:#f7814c;'>b</span> <span style='color:#fa9b58;'>b</span> <span style='color:#fdb365;'>b</span> <span style='color:#ecf7a6;'>e</span> <span style='color:#cbe982;'>p</span> <span style='color:#d1ec86;'>p</span> <span style='color:#ddf191;'>p</span> <span style='color:#d3ec87;'>p</span> <span style='color:#ecf7a6;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#d5ed88;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'p', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([ 0.0271,  1.6128, -0.0767, -0.1219, -0.4615,  0.3049, -0.1548,  0.0332,\n",
            "         0.0323, -0.4665, -0.4311, -0.4319,  0.0949,  0.2601,  0.2199,  0.1239,\n",
            "         0.2239,  0.1597,  0.1537,  0.1924], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fafdb8;'>s</span> <span style='color:#006837;'>p</span> <span style='color:#fff3ac;'>a</span> <span style='color:#feec9f;'>a</span> <span style='color:#fa9857;'>a</span> <span style='color:#bde379;'>a</span> <span style='color:#fee797;'>a</span> <span style='color:#f8fcb6;'>a</span> <span style='color:#f8fcb6;'>a</span> <span style='color:#fa9857;'>b</span> <span style='color:#fba35c;'>b</span> <span style='color:#fba35c;'>b</span> <span style='color:#ecf7a6;'>e</span> <span style='color:#c9e881;'>p</span> <span style='color:#d3ec87;'>p</span> <span style='color:#e8f59f;'>p</span> <span style='color:#d3ec87;'>p</span> <span style='color:#e0f295;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#daf08d;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.97954633, 0.99138793, 0.72103037, 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.99853902, 0.95470973, 0.67304883, 1.        ],\n",
              "       [0.9976163 , 0.92610534, 0.62506728, 1.        ],\n",
              "       [0.98039216, 0.59738562, 0.34117647, 1.        ],\n",
              "       [0.74117647, 0.8898885 , 0.47404844, 1.        ],\n",
              "       [0.99700115, 0.90703576, 0.59307958, 1.        ],\n",
              "       [0.97370242, 0.98892734, 0.71303345, 1.        ],\n",
              "       [0.97370242, 0.98892734, 0.71303345, 1.        ],\n",
              "       [0.98039216, 0.59738562, 0.34117647, 1.        ],\n",
              "       [0.98592849, 0.63737024, 0.35963091, 1.        ],\n",
              "       [0.98592849, 0.63737024, 0.35963091, 1.        ],\n",
              "       [0.92695117, 0.9692426 , 0.64905805, 1.        ],\n",
              "       [0.78823529, 0.91018839, 0.50449827, 1.        ],\n",
              "       [0.82745098, 0.92710496, 0.52987313, 1.        ],\n",
              "       [0.90941945, 0.96186082, 0.62506728, 1.        ],\n",
              "       [0.82745098, 0.92710496, 0.52987313, 1.        ],\n",
              "       [0.88019992, 0.94955786, 0.58508266, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.8568243 , 0.93971549, 0.55309496, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"spppppaaaaaaaabbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "model2 = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model2.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "vals4 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text4], dtype=torch.long))\n",
        "vals4 = vals4.view(400, 6).sum(axis=-1)\n",
        "\n",
        "format_chars(list(text4[0:50]), vals2[0:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7NaS9J9pOIFi",
        "outputId": "d25e4377-3e8f-484f-a1b6-b0a9cfe5e4d7"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'p', 'p', 'p', 'p', 'p', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([ 1.6211e-02,  1.4855e+00,  2.5426e-01,  5.0346e-02, -1.1846e+00,\n",
            "        -1.0383e+00, -8.2501e-01, -7.7528e-01, -4.3678e-01, -5.3587e-01,\n",
            "        -4.5938e-01, -3.8005e-01,  1.0128e-01,  2.5020e-01,  2.2745e-01,\n",
            "         1.7535e-01,  2.2096e-01,  9.8830e-02,  1.5501e-01,  2.1776e-01,\n",
            "         1.0477e-01,  7.6503e-02, -1.5959e-02,  4.7588e-02,  2.1500e-02,\n",
            "         5.9272e-02,  5.6882e-02,  1.8914e-02, -4.9604e-02,  6.7174e-03,\n",
            "         1.5475e-02,  6.5280e-02,  1.0134e-02,  2.7041e-02,  6.8334e-02,\n",
            "         6.3026e-03, -1.0717e-02,  4.0223e-02, -2.3945e-02, -5.3492e-02,\n",
            "        -3.6565e-02, -3.2506e-02,  8.0918e-04, -2.4274e-02, -4.6167e-02,\n",
            "        -4.6820e-04, -2.4270e-02, -5.9357e-02,  1.9951e-02, -4.5203e-02],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fbfdba;'>s</span> <span style='color:#006837;'>p</span> <span style='color:#cbe982;'>p</span> <span style='color:#f5fbb2;'>p</span> <span style='color:#a50026;'>p</span> <span style='color:#a50026;'>p</span> <span style='color:#d02927;'>a</span> <span style='color:#da362a;'>a</span> <span style='color:#fba35c;'>a</span> <span style='color:#f7814c;'>a</span> <span style='color:#fa9b58;'>a</span> <span style='color:#fdb365;'>a</span> <span style='color:#ecf7a6;'>a</span> <span style='color:#cbe982;'>a</span> <span style='color:#d1ec86;'>b</span> <span style='color:#ddf191;'>b</span> <span style='color:#d3ec87;'>b</span> <span style='color:#ecf7a6;'>b</span> <span style='color:#e2f397;'>b</span> <span style='color:#d5ed88;'>b</span> <span style='color:#ebf7a3;'>b</span> <span style='color:#f1f9ac;'>b</span> <span style='color:#fffcba;'>b</span> <span style='color:#f5fbb2;'>b</span> <span style='color:#fbfdba;'>b</span> <span style='color:#f4fab0;'>b</span> <span style='color:#f4fab0;'>e</span> <span style='color:#fbfdba;'>p</span> <span style='color:#fff7b2;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fdfebc;'>p</span> <span style='color:#f2faae;'>p</span> <span style='color:#fdfebc;'>p</span> <span style='color:#fafdb8;'>p</span> <span style='color:#f2faae;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fffdbc;'>p</span> <span style='color:#f7fcb4;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff7b2;'>p</span> <span style='color:#fffab6;'>p</span> <span style='color:#fffab6;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff8b4;'>p</span> <span style='color:#fffebe;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff6b0;'>p</span> <span style='color:#fbfdba;'>p</span> <span style='color:#fff8b4;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.79607843, 0.9135717 , 0.50957324, 1.        ],\n",
              "       [0.96201461, 0.98400615, 0.6970396 , 1.        ],\n",
              "       [0.64705882, 0.        , 0.14901961, 1.        ],\n",
              "       [0.64705882, 0.        , 0.14901961, 1.        ],\n",
              "       [0.81622453, 0.16239908, 0.15240292, 1.        ],\n",
              "       [0.85428681, 0.21168781, 0.16370627, 1.        ],\n",
              "       [0.98592849, 0.63737024, 0.35963091, 1.        ],\n",
              "       [0.96793541, 0.50742022, 0.29965398, 1.        ],\n",
              "       [0.98177624, 0.60738178, 0.34579008, 1.        ],\n",
              "       [0.99254133, 0.70157632, 0.39653979, 1.        ],\n",
              "       [0.92695117, 0.9692426 , 0.64905805, 1.        ],\n",
              "       [0.79607843, 0.9135717 , 0.50957324, 1.        ],\n",
              "       [0.81960784, 0.92372165, 0.52479815, 1.        ],\n",
              "       [0.86851211, 0.94463668, 0.56908881, 1.        ],\n",
              "       [0.82745098, 0.92710496, 0.52987313, 1.        ],\n",
              "       [0.92695117, 0.9692426 , 0.64905805, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ],\n",
              "       [0.92110727, 0.96678201, 0.64106113, 1.        ],\n",
              "       [0.94448289, 0.97662438, 0.67304883, 1.        ],\n",
              "       [0.99961553, 0.98808151, 0.7290273 , 1.        ],\n",
              "       [0.96201461, 0.98400615, 0.6970396 , 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.9561707 , 0.98154556, 0.68904268, 1.        ],\n",
              "       [0.9561707 , 0.98154556, 0.68904268, 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.99900038, 0.96901192, 0.6970396 , 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99123414, 0.99630911, 0.73702422, 1.        ],\n",
              "       [0.9503268 , 0.97908497, 0.68104575, 1.        ],\n",
              "       [0.99123414, 0.99630911, 0.73702422, 1.        ],\n",
              "       [0.97954633, 0.99138793, 0.72103037, 1.        ],\n",
              "       [0.9503268 , 0.97908497, 0.68104575, 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99976932, 0.9928489 , 0.73702422, 1.        ],\n",
              "       [0.96785852, 0.98646674, 0.70503652, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.99900038, 0.96901192, 0.6970396 , 1.        ],\n",
              "       [0.99930796, 0.97854671, 0.71303345, 1.        ],\n",
              "       [0.99930796, 0.97854671, 0.71303345, 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.99915417, 0.97377932, 0.70503652, 1.        ],\n",
              "       [0.99992311, 0.9976163 , 0.74502115, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.9988466 , 0.96424452, 0.68904268, 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.99915417, 0.97377932, 0.70503652, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str2 = \"ps\" + \"babaababaabaaabbabaae\" + \"p\"*(400 - 23)\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)))\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQA9U-F3zjSg",
        "outputId": "109dd764-eb35-4607-e0ef-f29a1b7bb085"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuNUMejaFKkY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}