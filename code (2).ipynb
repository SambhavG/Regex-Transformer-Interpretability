{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "RsOoab2nQHSw",
        "outputId": "ac719884-ad02-4a12-9617-e65cd33e1d56"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'str' object does not support item assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-154-39e8afb50c80>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_invalid_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-154-39e8afb50c80>\u001b[0m in \u001b[0;36mgenerate_invalid_string\u001b[0;34m(min_length, max_length)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mnum_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 20\n",
        "OOD_MAX_LENGTH = 40\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "def generate_valid(length):\n",
        "    num_a = random.randint(0, length)\n",
        "    num_b = length - num_a\n",
        "    return \"a\" * num_a + \"b\" * num_b\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "\n",
        "    if random.random() < 0.5:\n",
        "      while True:\n",
        "          # Random string of a's and b's which isn't a valid a*b* string\n",
        "          invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "          if \"ba\" in invalid_str:\n",
        "              break\n",
        "      num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "      return (\n",
        "          START_TOKEN\n",
        "          + PADDING_TOKEN * num_p\n",
        "          + invalid_str\n",
        "          + END_TOKEN\n",
        "          + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "      )\n",
        "    else:\n",
        "      string = generate_valid(length)\n",
        "      index = random.randint(0, length - 2)\n",
        "      string[index] = 'b'\n",
        "      string[index + 1] = 'a'\n",
        "      num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "      return (\n",
        "          START_TOKEN\n",
        "          + PADDING_TOKEN * num_p\n",
        "          + invalid_str\n",
        "          + END_TOKEN\n",
        "          + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "      )\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 1000  # Total number of samples\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "print(len(valid_dataset))\n",
        "print(len(invalid_dataset))\n",
        "\n",
        "split = len(valid_dataset) * 4 // 5;\n",
        "train_dataset = valid_dataset[:split] + invalid_dataset[:split]\n",
        "test_dataset = valid_dataset[split:] + invalid_dataset[split:]\n",
        "\n",
        "num_ood_samples = 1000\n",
        "dataset = []\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "ood_valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "ood_invalid_dataset = list(set(dataset))[:len(ood_valid_dataset)]\n",
        "print(len(ood_valid_dataset), len(ood_invalid_dataset))\n",
        "\n",
        "ood_dataset = ood_valid_dataset + ood_invalid_dataset\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(ood_dataset))\n",
        "\n",
        "\n",
        "# Write to file\n",
        "with open(\"train_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in train_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"test_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in test_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"ood_dataset_padded.txt\", \"w\") as f:\n",
        "    for data, label in ood_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 1\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class ActivationDatasetGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(ActivationDatasetGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        return x\n",
        "\n",
        "state_dict = torch.load('/content/1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth')\n",
        "\n",
        "datagen_model = ActivationDatasetGenerator(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "del state_dict['fc.weight']\n",
        "del state_dict['fc.bias']\n",
        "\n",
        "datagen_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2-QYYG-icxx",
        "outputId": "5691cd30-17c6-4762-e0c7-8e90cce3dfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "    while True:\n",
        "        # Random string of a's and b's which isn't a valid a*b* string\n",
        "        invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "        if \"ba\" in invalid_str:\n",
        "            break\n",
        "    num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + invalid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "    )\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 1000  # Total number of samples\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append(x)\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append(x)\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "\n",
        "dataset = invalid_dataset\n",
        "\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "def encode_string(string):\n",
        "    return [char_to_index[char] for char in string]\n",
        "\n",
        "class ActivationDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "    def add_data(self, string, tensor):\n",
        "        for i in range(len(string)):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            elif string[i - 1] == 'a' and string[i] == 'a':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([1]).long())\n",
        "            elif string[i - 1] == 'b' and string[i] == 'b':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([2]).long())\n",
        "            elif string[i - 1] == 'a' and string[i] == 'b':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([3]).long())\n",
        "            elif string[i - 1] == 'b' and string[i] == 'a':\n",
        "                self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                self.labels.append(torch.Tensor([4]).long())\n",
        "            else:\n",
        "                if i % 5 == 0:\n",
        "                    self.data.append(torch.cat((tensor[:,i-1], tensor[:,i])))\n",
        "                    self.labels.append(torch.Tensor([0]).long())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        return data, torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "activation_dataset = ActivationDataset()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(dataset)):\n",
        "      data_val = encode_string(dataset[i])\n",
        "      activation_val = datagen_model(torch.tensor(data_val, dtype=torch.long))\n",
        "      activation_dataset.add_data(dataset[i], activation_val)\n",
        "\n",
        "print(len(activation_dataset))\n",
        "print(activation_dataset.data[0])\n",
        "print(activation_dataset.labels[0])\n",
        "torch.save(activation_dataset, 'activations2_20000_split.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVIzRUXmkdnM",
        "outputId": "3a680fd4-1f0d-4605-cc1c-0e755cddb0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "203522\n",
            "tensor([[-2.0460, -0.1504,  1.6408, -0.4820, -0.8400,  1.7815],\n",
            "        [-2.0039, -0.3235,  1.5289, -0.5143, -0.6699,  1.9068]])\n",
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureProbe(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureProbe, self).__init__()\n",
        "        # lol this is tiny\n",
        "        self.mlp = nn.Linear(12, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x.reshape(len(x),-1))\n",
        "        return x\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "model = FeatureProbe()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "dataloader = DataLoader(activation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device).long().squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = torch.zeros(5).to(device)\n",
        "    total = torch.zeros(5).to(device)\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.argmax(torch.round(outputs).squeeze(), axis=-1)\n",
        "        # print(labels.shape)\n",
        "        # print(predicted.shape)\n",
        "        # atrocious code\n",
        "        for i in range(5):\n",
        "            total[i] += torch.sum(labels.to(device) == i).item()\n",
        "            correct[i] += torch.sum((predicted.squeeze().to(device) == labels.squeeze().to(device)) * (labels.squeeze().to(device) == i)).item()\n",
        "\n",
        "    print(correct)\n",
        "    print(total)\n",
        "    for i in range(5):\n",
        "        print(i, correct[i] / total[i], total[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEE-tEryruK2",
        "outputId": "5b7697c8-8731-4159-d950-6276b9f68240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-50228ac1b55b>:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return data, torch.tensor(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.38158777356147766\n",
            "tensor([44389., 39635., 39792., 36813., 36997.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9962, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1.0000, device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9995, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9260, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9305, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 2/5, Loss: 0.156574547290802\n",
            "tensor([44418., 39636., 39808., 39386., 39307.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9999, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9907, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9886, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 3/5, Loss: 0.1009211540222168\n",
            "tensor([44420., 39636., 39809., 39707., 39720.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(0.9999, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9987, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9990, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 4/5, Loss: 0.06118711829185486\n",
            "tensor([44421., 39636., 39810., 39753., 39739.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(1.0000, device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(0.9999, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9994, device='cuda:0') tensor(39761., device='cuda:0')\n",
            "Epoch 5/5, Loss: 0.041719596832990646\n",
            "tensor([44421., 39636., 39811., 39756., 39753.], device='cuda:0')\n",
            "tensor([44557., 39636., 39811., 39757., 39761.], device='cuda:0')\n",
            "0 tensor(0.9969, device='cuda:0') tensor(44557., device='cuda:0')\n",
            "1 tensor(1., device='cuda:0') tensor(39636., device='cuda:0')\n",
            "2 tensor(1., device='cuda:0') tensor(39811., device='cuda:0')\n",
            "3 tensor(1.0000, device='cuda:0') tensor(39757., device='cuda:0')\n",
            "4 tensor(0.9998, device='cuda:0') tensor(39761., device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um9d6gaFQHS2",
        "outputId": "6f8fb310-c53b-4460-f059-83a83611d30c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.0643521100282669\n",
            "Accuracy: 0.9852941176470589\n",
            "Epoch 2/5, Loss: 0.09316340833902359\n",
            "Accuracy: 0.9913097454996896\n",
            "Epoch 3/5, Loss: 0.0352286621928215\n",
            "Accuracy: 0.9910724066162778\n",
            "Epoch 4/5, Loss: 0.025056665763258934\n",
            "Accuracy: 0.9913827728484318\n",
            "Epoch 5/5, Loss: 0.04972020909190178\n",
            "Accuracy: 0.9928341914046811\n",
            "Epoch 1/5, Loss: 0.02033993974328041\n",
            "Accuracy: 0.9934731807061744\n",
            "Epoch 2/5, Loss: 0.028123032301664352\n",
            "Accuracy: 0.9935827217292876\n",
            "Epoch 3/5, Loss: 0.03850043565034866\n",
            "Accuracy: 0.9931719428926132\n",
            "Epoch 4/5, Loss: 0.022266890853643417\n",
            "Accuracy: 0.9941943257750028\n",
            "Epoch 5/5, Loss: 0.04888172447681427\n",
            "Accuracy: 0.9939296016358126\n",
            "Accuracy: 0.9940484883890756\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants for model\n",
        "# VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 100\n",
        "OOD_MAX_LENGTH = 500\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = StringDataset(\"train_dataset_padded (1).txt\")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"1head_1layer_embed6revbatch512hidden1_100max500ood_total_transformer_model.pth\")\n",
        "\n",
        "test_dataset = StringDataset(\"test_dataset_padded (1).txt\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in test_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgtAxiKqojlD",
        "outputId": "ecb9b0c7-f8e8-4b59-dcc5-e95c00be44f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9719818488813845\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_padded (1).txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQF3oa6oQHS5",
        "outputId": "ef5f5146-b590-4bc3-f1a5-57de88f25be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 examples which are wrongly classified\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababbaaaaaaaaaabbbbaaaabaababaabaabbbbaabaabbbbabaaaabbaababbaaaabbabbbabaabbababbababbabbaaaaabbabaaabbaaaaaabaaabbbaabbaaabbaaaabaababaaaabbaaabbbbabaabbabaaaabbbababaaaaabbabaababbbaaaabbaaabbbabaaabbbbabbbaaaababbbabbbaababaabbbabaaabbaabbbbaaababbaaabaabbbbaaabbabaaaaaabaaabaabbbbbaaaaaabaababbbbbabaaabbbabbbabbbbbbbabbbaabbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaabbbbaababaabababbbbbbbbbbabaabbbabbbaabbaabbbaabbbabbabbabbbbaaabbababbaabbbabbaaabbabbbbabbbbabbaaabaabaaaabaabbababaaababababbbbaaababaaaaabaabbbaaabbabaababbababaabaaabbaaabbbbaabbbbbbbbababaaaabaabaabaaababbbabbbbabaabaabbbbaaaabaabaaaabbbbabaaaabbbabbbbbababbabbabaababbbabaaaabbbbaabbbaabaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbabaaaaabaababbabbaabbabbbbaabbabbaaabbababbbbbbbabaabbbaaaaaababaaaaaaaabbabbbababbaabbabaabbaaaaabaabbabbaaaaaaabbabbabbbaaabbabbbbabbabbbbaabbbbaaaaaabaaaaaabbbabaaaabaaaababaaabepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaabbaaaaabbabbbbaababaaaababbabbbaaaaabbababbababaaabbaababbbbabbaaabaabbbaabbbbbaaabababbabbbaaabbbabbaabbaaaabbabbabaabbbabbbbbabaaaaaabbaabbaababaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbabaaabbaaababaabbaaabaaabbaababbabbbbbaabaabbbbaabbaabaabbbbbabaaaabbabaabaabaabbaaabababbbabaaabaaaababbbabaabaaaaabaaaaaaabbaaaaaaabababbbbbabaabbabbbababbbbbbaaabaaaaabaaababbaaaaaababababaabbabbbbbbbabbbababbbaabbbbbabbaabbbbbabbbbbaabababbbbbbbbaabaabbaaaabbbabaabaabaabbbbbaabbbbbbbabbbabababbaaaababbbbbabababaabbbaaaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaabbabaababaaaababbbabbbbaaabbbbaaababababbababbbaabbbabaabbababbabbbbbababbababbabbbaabbabbaabbbbabbbaaabbbbabbabaabbaababaabaabaabbabaaaabbabbbaaaabaaaabaaabaabaaaaababbbabaaababbabababbbbaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababaaabbaabaabbbaaababbaaaabbbbaabbbbaaababbaabaabaaaabaabbaaababaabbbbabbaababbaabaababaaabbabbbbbaaabaaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaabaabaaabbbbabbbabaaabababbaaaaaababbabaabbabbbbbbbababababaaabaababaabbaaaabaabbabbbabbbbabbbaaaaabababbbbabbabbbaaabbaabeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaababbaaabbbabbbaabaaababaaabaabbaaababbaaaabbaabbabbabbaabbabaabaaaaaababbababbbababbabbabbbaaaaabbbbbbababaaababbbabbaaaabbbbabbabaaaaaabbbbabbbaaabbaaaabbaaabaabbabbbabaaaaabbbabbababaaabbaaaabbbabaabaaaaabbaababbbbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaababbaaaaabaaaababbababbbbababaabaabaabbbbbbaaaaabbaabbabaababbaabbbbbbbbbabbbaabaaabbabaabbabbaababbbaabbbbaababbbaaabbbbbaaabbbbbaaaabbbbbbbaaabbbabbbaabbaaabbbababbbaababababaaabbabaaabaabbaababaaabbaabababaabababaaaabaabbbabaaaabaaabbaaaabababbaaabbaabaaaababababaabababbaaababaaabaabbababbabbbbbbbbbaaaaabaabbbabbbaaaabbabbbbbabaabbbababaabaabababbbabbaaaaaabaabbbabaaabbaabbaabaababbbabaaaabaabaababbbbababbbbabaaabaaabaababbbabbaabbbabaaaabbaaababbbabbaabaababbabababbbabaabbaabbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbaabaabbabbbbaababbbaaaaababbbaababbaabbaababaababaaabababbaaabababababbbbbbbaaababbaaabaabbaabaaabaabbbbaababbbbaaaaaaaabaabbaaabbabaababbbabababaabbbaabbaaabbabaaabbababaaaabbbaabaabbbbabbbbaaabbbbaabbababbbababbababababaabaaaaaaaabbabaababababbbbababaababaaaabaaaabababbbabbbabbbaaabababbabbabbabaaaabbbabaaababbbaaaaaabaaabaaaaababababbabaababbbaaaabaaaaabbabaababaaaaababaabaaabaabababaababbaababbabbbaaabbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaaabbbabbabbbbbbaaabaaaaabbabaaabaabbaaaabaabbbaabaabbbbbbbbabaabbbbbaabbbaabbaabbaabbbbababaaabbaaabbbbbaabbaabbababaaabbaaabbaaaabbaababbaaaaababaabaaababaabbbbbaabaabbabbbbababbbbbaababbaababbbbbaabbababbaaaaaabbbababaabaaaaaaabbaabababaaabbaababbbbbbaabbabbbaabababbbbbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n"
          ]
        }
      ],
      "source": [
        "# Print 20 examples which are wrongly classified\n",
        "print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        if count == 20:\n",
        "            break\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            string = \"\".join([VALID_CHARACTERS[int(idx)] for idx in inputs[i]])\n",
        "            print(string, labels[i].item(), predicted[i].item())\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0WOiMFYFLKF",
        "outputId": "cb6efb8a-2ed4-403a-84f6-7b829b029a7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5.4745e+04 6.6200e+02]\n",
            " [2.9000e+01 5.4112e+04]]\n",
            "[691.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "model_2 = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_2.load_state_dict(torch.load('1head_1layer_embed6revbatch512hidden1_100max500ood_total_transformer_model.pth'))\n",
        "model_2.to(device)\n",
        "\n",
        "# Print 20 examples which are wrongly classified\n",
        "# print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "matrix = np.zeros((2, 2))\n",
        "num_pad_zeros = np.zeros((500))\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        matrix[int(predicted[i]), int(labels[i])] += 1\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            num_start_pad = 0\n",
        "            for j in range(len(inputs[i])):\n",
        "                if VALID_CHARACTERS[int(inputs[i][j])] != 'p':\n",
        "                    break\n",
        "                num_start_pad += 1\n",
        "            num_pad_zeros[num_start_pad] += 1\n",
        "print(matrix)\n",
        "print(num_pad_zeros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ekqsmo6JWde",
        "outputId": "6057ec8a-00aa-4f05-fe39-bcdef45b8e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9748629848229342\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_gappadded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model_2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtNf76gYGIIe",
        "outputId": "3ae629bc-80b3-40f3-c7f7-6ddc0f6f81f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1173, -0.4868, -0.2787, -0.3836,  0.4869,  0.4462],\n",
            "        [-0.3077,  0.1080, -0.0703,  0.2785,  0.0020,  0.3271],\n",
            "        [-0.3017, -0.1836, -0.0864, -0.2321, -0.2330, -0.4434],\n",
            "        [-0.0861,  0.6321, -0.4590, -0.3281, -0.5029, -0.4216],\n",
            "        [ 0.4099, -0.1211, -0.1174, -0.1267, -0.3700, -0.1985],\n",
            "        [ 0.3000,  0.4385, -0.3269,  0.3167,  0.4170, -0.3065],\n",
            "        [ 0.1610, -0.0720,  0.3529,  0.4052,  0.0425,  0.2735],\n",
            "        [-0.3191, -0.0640,  0.2417, -0.0950,  0.0050, -0.3506],\n",
            "        [-0.3651, -0.1423, -0.1854, -0.2698,  0.1397,  0.0741],\n",
            "        [ 0.4333,  0.2350, -0.0254,  0.4441,  0.2188,  0.1552],\n",
            "        [ 0.1111, -0.1373, -0.0707,  0.4105,  0.1355,  0.3309],\n",
            "        [-0.2359, -0.2420, -0.0904,  0.2508,  0.3944, -0.0200],\n",
            "        [-0.2139,  0.3799, -0.1082, -0.3886,  0.0216, -0.0505],\n",
            "        [-0.2125,  0.3423, -0.3185,  0.3012, -0.3671,  0.2281],\n",
            "        [-0.4219,  0.1814,  0.2821,  0.0127,  0.1133,  0.3603],\n",
            "        [-0.2622, -0.2055,  0.4117,  0.4700,  0.1935,  0.2158],\n",
            "        [ 0.0299, -0.0371, -0.0226,  0.0525,  0.0328, -0.4088],\n",
            "        [-0.1115,  0.3936, -0.3476,  0.4100, -0.1752,  0.3781]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(model_2.transformer_encoder.layers[0].self_attn.in_proj_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2F50v9OH4EL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Transformer model\n",
        "class TransformerDebugClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerDebugClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layer = TransformerEncoderLayerWithAttention(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoderWithAttention(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        print(x.shape)\n",
        "        x, attn = self.transformer_encoder(x)\n",
        "        print(x.shape)\n",
        "        print(attn)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvrxJ8MMIZOV",
        "outputId": "23b4fc1b-c5f9-4386-db0a-d687f3aaac48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9727547931382442\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model_3 = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_3.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "ood_dataset = StringDataset(\"ood_dataset_padded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model_3.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "device = 'cpu'\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_3(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1v80IUlUNPFH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "bea3821d-6252-453d-b7e0-2430b4adfdd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-71a3ce8f2a70>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msym_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1930\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1931\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/func/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_and_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional_call\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_module_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm_replacement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_all_batch_norm_modules_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mpath_stats\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def is_valid(string):\n",
        "  return (\"ba\" not in string)\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "class AdversaryNet(nn.Module):\n",
        "    def __init__(self, x):\n",
        "        super(AdversaryNet, self).__init__()\n",
        "        self.logits = x\n",
        "\n",
        "    def forward(self):\n",
        "        # sigmoid to sum to 1, batchsize\n",
        "        probs = torch.sigmoid(self.logits)\n",
        "        result = torch.zeros(5,len(self.logits)).long()\n",
        "        log_prob = torch.zeros(5)\n",
        "        for i in range(5):\n",
        "            curr_result = torch.bernoulli(probs).long()\n",
        "            log_prob[i] = torch.sum(torch.log(probs * curr_result + (1 - probs) * (1 - curr_result)))\n",
        "            result[i,:] = curr_result\n",
        "        return result, log_prob\n",
        "\n",
        "def make_fooling_character(X, model, string_length=20):\n",
        "    \"\"\"\n",
        "    Generate a fooling distribution: an invalid string that the model classifies\n",
        "    as valid. In this case, start with random noise and perform gradient ascent.\n",
        "\n",
        "    Input: Tensor of shape (string_length) between [0, 1], sample a and b from that\n",
        "    Model: Pretrained predictor model\n",
        "\n",
        "    Returns: Fooling distribution\n",
        "    \"\"\"\n",
        "\n",
        "    net = AdversaryNet(X)\n",
        "    optimizer = optim.Adam([net.logits], lr=0.01)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        # Generate an episode\n",
        "        actions, log_prob = net()\n",
        "\n",
        "        string = torch.zeros(5, OOD_MAX_LENGTH).long()\n",
        "        string[:,0] = 0\n",
        "        string[:,1:string_length + 1] = actions + 1\n",
        "        string[:,string_length + 1] = 3\n",
        "        string[:,string_length + 2:] = 4\n",
        "\n",
        "        # Simulate the environment and get rewards\n",
        "        rewards = model.forward(string)  # Reward based on sampled actions\n",
        "        # print(rewards)\n",
        "\n",
        "        # Calculate the policy gradient\n",
        "        policy_gradient = torch.zeros_like(log_prob)\n",
        "        for i in range(5):\n",
        "          policy_gradient[i] = (rewards[i][0]) * log_prob[i]\n",
        "        policy_gradient = policy_gradient.sum()\n",
        "\n",
        "        # Update the policy network\n",
        "        optimizer.zero_grad()\n",
        "        policy_gradient.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if episode % 100 == 0:\n",
        "            print(f'Episode {episode}: Policy Gradient = {policy_gradient.item()}')\n",
        "            print(net.logits)\n",
        "            print(rewards.mean())\n",
        "\n",
        "make_fooling_character(torch.zeros(20).requires_grad_(), model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "def is_valid(string):\n",
        "  return (\"ba\" not in string)\n",
        "\n",
        "def generate_strings(length):\n",
        "    # Define the characters\n",
        "    characters = ['a', 'b']\n",
        "\n",
        "    strings = []\n",
        "    # Generate all combinations of characters of given length\n",
        "    for i in range(length):\n",
        "      strings.append(\"a\" * i + \"b\" * (length - i))\n",
        "\n",
        "    for i in range(length):\n",
        "      strings.append(random.choices(MAIN_CHARACTERS, k=length))\n",
        "\n",
        "    return strings\n",
        "\n",
        "# Generate all strings of length 8\n",
        "strings_length_8 = generate_strings(120)\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "for string in strings_length_8:\n",
        "    str2 = \"s\" + string + \"e\" + \"p\"*(400 - 122)\n",
        "    if (model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)) < 0.3 and is_valid(str2)):\n",
        "      print(str2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF-Inn-j7cRm",
        "outputId": "6b2876d2-c7c2-4f34-c2cf-084ae1f160e0"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "sabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "VALID_CHARACTERS = [\"p\", \"e\", \"b\", \"a\", \"s\"]\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "\n",
        "def is_valid(string):\n",
        "  return (\"ba\" not in string)\n",
        "\n",
        "text = \"psaaaabaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "\n",
        "class TransformerClassifier2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "    def return_pre_fc(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        sum = self.fc.weight.data.squeeze() * x\n",
        "        return sum\n",
        "\n",
        "model = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "def generate_strings(length):\n",
        "    # Define the characters\n",
        "    characters = ['a', 'b']\n",
        "\n",
        "    strings = []\n",
        "    # Generate all combinations of characters of given length\n",
        "    for i in range(length):\n",
        "      for j in range(10):\n",
        "        strings.append(\"a\" * i + \"b\" * (length - i))\n",
        "\n",
        "    for i in range(length * 10):\n",
        "      strings.append(random.choices(MAIN_CHARACTERS, k=length))\n",
        "\n",
        "    return strings\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "vals = model.return_pre_fc(torch.tensor([char_to_index[char] for char in text], dtype=torch.long))\n",
        "vals = vals.view(400, 6).sum(axis=-1)\n",
        "\n",
        "# 5 = 0.0513, 10 = 0.0214, 20 = -0.0419, 40 = 0.0045\n",
        "# 5 = -0.0104, 10 = 0.0790,\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in text], dtype=torch.long)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJozdr3VE5SP",
        "outputId": "0b1a1c29-beac-46a8-c32d-28b5feb2a05f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8653]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as mcolors\n",
        "from matplotlib import cm\n",
        "import numpy as np\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def format(value):\n",
        "  c, l = value\n",
        "  return \"<span style='color:{};'>{}</span>\".format(l,c)\n",
        "\n",
        "def format_chars(chars,numbers):\n",
        "    print(chars)\n",
        "    print(numbers)\n",
        "    numbers = np.array(numbers.detach()).astype(float)\n",
        "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
        "    cmap = cm.RdYlGn\n",
        "    colors = cmap(norm(numbers))\n",
        "    hexcolor = [mcolors.to_hex(c) for c in colors]\n",
        "    text = \" \".join(list(map(format, zip(chars,hexcolor))))\n",
        "    text = \"<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'>\" + text + \"</div>\"\n",
        "    display(HTML(text))\n",
        "    return colors\n",
        "\n",
        "format_chars(list(text[0:20]), vals[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "5fHKpV-gHmuV",
        "outputId": "2df00d57-55e0-4175-f09d-daa7692f7c2e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p', 's', 'a', 'a', 'a', 'a', 'b', 'a', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([-0.0086,  1.9368, -0.2381, -0.1936,  0.1485, -0.0078, -0.8250,  0.1185,\n",
            "         0.0331,  0.2427,  0.4280,  0.4146,  0.3182,  0.2677,  0.2449,  0.1272,\n",
            "         0.2109,  0.1644,  0.1540,  0.2193], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fffdbc;'>p</span> <span style='color:#006837;'>s</span> <span style='color:#fed683;'>a</span> <span style='color:#fee18d;'>a</span> <span style='color:#e2f397;'>a</span> <span style='color:#fffebe;'>a</span> <span style='color:#d02927;'>b</span> <span style='color:#e8f59f;'>a</span> <span style='color:#f8fcb6;'>e</span> <span style='color:#cdea83;'>p</span> <span style='color:#9dd569;'>p</span> <span style='color:#a0d669;'>p</span> <span style='color:#bbe278;'>p</span> <span style='color:#c7e77f;'>p</span> <span style='color:#cdea83;'>p</span> <span style='color:#e6f59d;'>p</span> <span style='color:#d7ee8a;'>p</span> <span style='color:#dff293;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#d3ec87;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99976932, 0.9928489 , 0.73702422, 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.9953095 , 0.83998462, 0.51280277, 1.        ],\n",
              "       [0.99623222, 0.88319877, 0.55309496, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.99992311, 0.9976163 , 0.74502115, 1.        ],\n",
              "       [0.81622453, 0.16239908, 0.15240292, 1.        ],\n",
              "       [0.90941945, 0.96186082, 0.62506728, 1.        ],\n",
              "       [0.97370242, 0.98892734, 0.71303345, 1.        ],\n",
              "       [0.80392157, 0.91695502, 0.51464821, 1.        ],\n",
              "       [0.6165321 , 0.83590927, 0.41191849, 1.        ],\n",
              "       [0.62637447, 0.8402153 , 0.412995  , 1.        ],\n",
              "       [0.73333333, 0.88650519, 0.46897347, 1.        ],\n",
              "       [0.78039216, 0.90680507, 0.4994233 , 1.        ],\n",
              "       [0.80392157, 0.91695502, 0.51464821, 1.        ],\n",
              "       [0.90357555, 0.95940023, 0.61707036, 1.        ],\n",
              "       [0.84313725, 0.93387159, 0.54002307, 1.        ],\n",
              "       [0.87435602, 0.94709727, 0.57708574, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.82745098, 0.92710496, 0.52987313, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"psaabbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "text3 = \"psaaaaaaabbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "model2 = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model2.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "vals2 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text2], dtype=torch.long))\n",
        "vals3 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text3], dtype=torch.long))\n",
        "vals2 = vals2.view(400, 6).sum(axis=-1)\n",
        "vals3 = vals3.view(400, 6).sum(axis=-1)\n",
        "\n",
        "format_chars(list(text2[0:20]), vals2[0:20])\n",
        "format_chars(list(text3[0:20]), vals3[0:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "dD1CXQPWM-Cx",
        "outputId": "7d066a7f-5aac-4c81-b84b-a95fd8c27a64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p', 's', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([-0.0061,  2.2521, -0.3051,  0.0503, -1.3641, -0.9924, -0.7827, -0.7201,\n",
            "        -0.5794, -0.4738, -0.4125, -0.3607,  0.1496,  0.2707,  0.0803,  0.1730,\n",
            "         0.2441,  0.1406,  0.1537,  0.1993], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fffebe;'>p</span> <span style='color:#006837;'>s</span> <span style='color:#fdc574;'>a</span> <span style='color:#f5fbb2;'>a</span> <span style='color:#a50026;'>b</span> <span style='color:#a50026;'>b</span> <span style='color:#d93429;'>b</span> <span style='color:#e24731;'>b</span> <span style='color:#f57245;'>b</span> <span style='color:#fa9656;'>b</span> <span style='color:#fcaa5f;'>b</span> <span style='color:#fdb768;'>b</span> <span style='color:#e2f397;'>e</span> <span style='color:#c7e77f;'>p</span> <span style='color:#eff8aa;'>p</span> <span style='color:#ddf191;'>p</span> <span style='color:#cdea83;'>p</span> <span style='color:#e5f49b;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#d9ef8b;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['p', 's', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([-0.0060,  2.2022, -0.3051,  0.0890,  0.2074,  0.2694,  0.0038,  0.1553,\n",
            "        -0.0698, -0.4840, -0.4485, -0.3800,  0.0558,  0.2363,  0.2157,  0.1707,\n",
            "         0.2435,  0.1448,  0.1550,  0.2178], grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fffebe;'>p</span> <span style='color:#006837;'>s</span> <span style='color:#fdc574;'>a</span> <span style='color:#eef8a8;'>a</span> <span style='color:#d7ee8a;'>a</span> <span style='color:#c7e77f;'>a</span> <span style='color:#feffbe;'>a</span> <span style='color:#e2f397;'>a</span> <span style='color:#fff5ae;'>a</span> <span style='color:#f99355;'>b</span> <span style='color:#fb9d59;'>b</span> <span style='color:#fdb365;'>b</span> <span style='color:#f4fab0;'>e</span> <span style='color:#cfeb85;'>p</span> <span style='color:#d5ed88;'>p</span> <span style='color:#dff293;'>p</span> <span style='color:#cdea83;'>p</span> <span style='color:#e3f399;'>p</span> <span style='color:#e2f397;'>p</span> <span style='color:#d5ed88;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.99992311, 0.9976163 , 0.74502115, 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.99392541, 0.77078047, 0.45467128, 1.        ],\n",
              "       [0.93279508, 0.97170319, 0.65705498, 1.        ],\n",
              "       [0.84313725, 0.93387159, 0.54002307, 1.        ],\n",
              "       [0.78039216, 0.90680507, 0.4994233 , 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.99869281, 0.95947712, 0.68104575, 1.        ],\n",
              "       [0.97762399, 0.57739331, 0.33194925, 1.        ],\n",
              "       [0.98316032, 0.61737793, 0.35040369, 1.        ],\n",
              "       [0.99254133, 0.70157632, 0.39653979, 1.        ],\n",
              "       [0.9561707 , 0.98154556, 0.68904268, 1.        ],\n",
              "       [0.81176471, 0.92033833, 0.51972318, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ],\n",
              "       [0.87435602, 0.94709727, 0.57708574, 1.        ],\n",
              "       [0.80392157, 0.91695502, 0.51464821, 1.        ],\n",
              "       [0.89188774, 0.95447905, 0.60107651, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text4 = \"spppppaaaaaaaabbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp\"\n",
        "model2 = TransformerClassifier2(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model2.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "vals4 = model2.return_pre_fc(torch.tensor([char_to_index[char] for char in text4], dtype=torch.long))\n",
        "vals4 = vals4.view(400, 6).sum(axis=-1)\n",
        "\n",
        "print(vals2[:7])\n",
        "format_chars(list(text4[0:50]), vals2[0:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7NaS9J9pOIFi",
        "outputId": "4febe3d5-e195-4e28-e497-8602d9485b1d"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0162,  1.4855,  0.2543,  0.0503, -1.1846, -1.0383, -0.8250],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "['s', 'p', 'p', 'p', 'p', 'p', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'e', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p', 'p']\n",
            "tensor([ 1.6211e-02,  1.4855e+00,  2.5426e-01,  5.0346e-02, -1.1846e+00,\n",
            "        -1.0383e+00, -8.2501e-01, -7.7528e-01, -4.3678e-01, -5.3587e-01,\n",
            "        -4.5938e-01, -3.8005e-01,  1.0128e-01,  2.5020e-01,  2.2745e-01,\n",
            "         1.7535e-01,  2.2096e-01,  9.8830e-02,  1.5501e-01,  2.1776e-01,\n",
            "         1.0477e-01,  7.6503e-02, -1.5959e-02,  4.7588e-02,  2.1500e-02,\n",
            "         5.9272e-02,  5.6882e-02,  1.8914e-02, -4.9604e-02,  6.7174e-03,\n",
            "         1.5475e-02,  6.5280e-02,  1.0134e-02,  2.7041e-02,  6.8334e-02,\n",
            "         6.3026e-03, -1.0717e-02,  4.0223e-02, -2.3945e-02, -5.3492e-02,\n",
            "        -3.6565e-02, -3.2506e-02,  8.0918e-04, -2.4274e-02, -4.6167e-02,\n",
            "        -4.6820e-04, -2.4270e-02, -5.9357e-02,  1.9951e-02, -4.5203e-02],\n",
            "       grad_fn=<SliceBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style='font-size:14pt;font-weight:bold;background-color:#000000;padding:8px'><span style='color:#fbfdba;'>s</span> <span style='color:#006837;'>p</span> <span style='color:#cbe982;'>p</span> <span style='color:#f5fbb2;'>p</span> <span style='color:#a50026;'>p</span> <span style='color:#a50026;'>p</span> <span style='color:#d02927;'>a</span> <span style='color:#da362a;'>a</span> <span style='color:#fba35c;'>a</span> <span style='color:#f7814c;'>a</span> <span style='color:#fa9b58;'>a</span> <span style='color:#fdb365;'>a</span> <span style='color:#ecf7a6;'>a</span> <span style='color:#cbe982;'>a</span> <span style='color:#d1ec86;'>b</span> <span style='color:#ddf191;'>b</span> <span style='color:#d3ec87;'>b</span> <span style='color:#ecf7a6;'>b</span> <span style='color:#e2f397;'>b</span> <span style='color:#d5ed88;'>b</span> <span style='color:#ebf7a3;'>b</span> <span style='color:#f1f9ac;'>b</span> <span style='color:#fffcba;'>b</span> <span style='color:#f5fbb2;'>b</span> <span style='color:#fbfdba;'>b</span> <span style='color:#f4fab0;'>b</span> <span style='color:#f4fab0;'>e</span> <span style='color:#fbfdba;'>p</span> <span style='color:#fff7b2;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fdfebc;'>p</span> <span style='color:#f2faae;'>p</span> <span style='color:#fdfebc;'>p</span> <span style='color:#fafdb8;'>p</span> <span style='color:#f2faae;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fffdbc;'>p</span> <span style='color:#f7fcb4;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff7b2;'>p</span> <span style='color:#fffab6;'>p</span> <span style='color:#fffab6;'>p</span> <span style='color:#feffbe;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff8b4;'>p</span> <span style='color:#fffebe;'>p</span> <span style='color:#fffbb8;'>p</span> <span style='color:#fff6b0;'>p</span> <span style='color:#fbfdba;'>p</span> <span style='color:#fff8b4;'>p</span></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.        , 0.40784314, 0.21568627, 1.        ],\n",
              "       [0.79607843, 0.9135717 , 0.50957324, 1.        ],\n",
              "       [0.96201461, 0.98400615, 0.6970396 , 1.        ],\n",
              "       [0.64705882, 0.        , 0.14901961, 1.        ],\n",
              "       [0.64705882, 0.        , 0.14901961, 1.        ],\n",
              "       [0.81622453, 0.16239908, 0.15240292, 1.        ],\n",
              "       [0.85428681, 0.21168781, 0.16370627, 1.        ],\n",
              "       [0.98592849, 0.63737024, 0.35963091, 1.        ],\n",
              "       [0.96793541, 0.50742022, 0.29965398, 1.        ],\n",
              "       [0.98177624, 0.60738178, 0.34579008, 1.        ],\n",
              "       [0.99254133, 0.70157632, 0.39653979, 1.        ],\n",
              "       [0.92695117, 0.9692426 , 0.64905805, 1.        ],\n",
              "       [0.79607843, 0.9135717 , 0.50957324, 1.        ],\n",
              "       [0.81960784, 0.92372165, 0.52479815, 1.        ],\n",
              "       [0.86851211, 0.94463668, 0.56908881, 1.        ],\n",
              "       [0.82745098, 0.92710496, 0.52987313, 1.        ],\n",
              "       [0.92695117, 0.9692426 , 0.64905805, 1.        ],\n",
              "       [0.88604383, 0.95201845, 0.59307958, 1.        ],\n",
              "       [0.83529412, 0.93048827, 0.5349481 , 1.        ],\n",
              "       [0.92110727, 0.96678201, 0.64106113, 1.        ],\n",
              "       [0.94448289, 0.97662438, 0.67304883, 1.        ],\n",
              "       [0.99961553, 0.98808151, 0.7290273 , 1.        ],\n",
              "       [0.96201461, 0.98400615, 0.6970396 , 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.9561707 , 0.98154556, 0.68904268, 1.        ],\n",
              "       [0.9561707 , 0.98154556, 0.68904268, 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.99900038, 0.96901192, 0.6970396 , 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99123414, 0.99630911, 0.73702422, 1.        ],\n",
              "       [0.9503268 , 0.97908497, 0.68104575, 1.        ],\n",
              "       [0.99123414, 0.99630911, 0.73702422, 1.        ],\n",
              "       [0.97954633, 0.99138793, 0.72103037, 1.        ],\n",
              "       [0.9503268 , 0.97908497, 0.68104575, 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99976932, 0.9928489 , 0.73702422, 1.        ],\n",
              "       [0.96785852, 0.98646674, 0.70503652, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.99900038, 0.96901192, 0.6970396 , 1.        ],\n",
              "       [0.99930796, 0.97854671, 0.71303345, 1.        ],\n",
              "       [0.99930796, 0.97854671, 0.71303345, 1.        ],\n",
              "       [0.99707805, 0.9987697 , 0.74502115, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.99915417, 0.97377932, 0.70503652, 1.        ],\n",
              "       [0.99992311, 0.9976163 , 0.74502115, 1.        ],\n",
              "       [0.99946175, 0.98331411, 0.72103037, 1.        ],\n",
              "       [0.9988466 , 0.96424452, 0.68904268, 1.        ],\n",
              "       [0.98539023, 0.99384852, 0.7290273 , 1.        ],\n",
              "       [0.99915417, 0.97377932, 0.70503652, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str2 = \"ps\" + \"babaababaabaaabbabaae\" + \"p\"*(400 - 23)\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)))\n",
        "print(model.forward(torch.tensor([char_to_index[char] for char in str2], dtype=torch.long)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQA9U-F3zjSg",
        "outputId": "109dd764-eb35-4607-e0ef-f29a1b7bb085"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuNUMejaFKkY"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}