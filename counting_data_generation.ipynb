{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a simple countable dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15000/15000 [00:23<00:00, 631.56it/s] \n",
            "100%|██████████| 15000/15000 [00:24<00:00, 610.90it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(valid_dataset): 15000\n",
            "len(invalid_dataset): 15000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 15000/15000 [00:13<00:00, 1089.70it/s]\n",
            "100%|██████████| 15000/15000 [00:12<00:00, 1220.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(ood_valid_dataset):  15000 len(ood_invalid_dataset):  15000\n",
            "len(train_dataset):  24000\n",
            "len(test_dataset):  6000\n",
            "len(ood_dataset):  30000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "\n",
        "    ab = random.choice(['a', 'b'])\n",
        "    if ab == 'a':\n",
        "        num_a = random.choice(range(0, max_length - 2, 2))\n",
        "        valid_str = \"a\" * num_a\n",
        "\n",
        "    else:\n",
        "        num_b = random.choice(range(0, max_length - 2, 2))\n",
        "        valid_str = \"b\" * num_b\n",
        "\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        PADDING_TOKEN * num_p\n",
        "        + START_TOKEN\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "\n",
        "    ab = random.choice(['a', 'b'])\n",
        "    if ab == 'a':\n",
        "        num_a = random.choice(range(1, max_length - 3, 2))\n",
        "        valid_str = \"a\" * num_a\n",
        "\n",
        "    else:\n",
        "        num_b = random.choice(range(1, max_length - 3, 2))\n",
        "        valid_str = \"b\" * num_b\n",
        "\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        PADDING_TOKEN * num_p\n",
        "        + START_TOKEN\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 15000  # Total number of samples\n",
        "\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        if (x, 1) not in dataset:\n",
        "            dataset.append((x, 1))\n",
        "            break\n",
        "\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        if (x, 0) not in dataset:\n",
        "            dataset.append((x, 0))\n",
        "            break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "print(f'len(valid_dataset): {len(valid_dataset)}')\n",
        "print(f'len(invalid_dataset): {len(invalid_dataset)}')\n",
        "\n",
        "split = len(valid_dataset) * 4 // 5\n",
        "train_dataset = valid_dataset[:split] + invalid_dataset[:split]\n",
        "test_dataset = valid_dataset[split:] + invalid_dataset[split:]\n",
        "\n",
        "num_ood_samples = 15000\n",
        "dataset = []\n",
        "for _ in tqdm(range(num_ood_samples)):\n",
        "    while True:\n",
        "        x = generate_valid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        if (x, 1) not in dataset:\n",
        "            dataset.append((x, 1))\n",
        "            break\n",
        "\n",
        "# remove duplicates\n",
        "ood_valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in tqdm(range(num_ood_samples)):\n",
        "    while True:\n",
        "        x = generate_invalid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        if (x, 0) not in dataset:\n",
        "            dataset.append((x, 0))\n",
        "            break\n",
        "\n",
        "# Remove all duplicates\n",
        "ood_invalid_dataset = list(set(dataset))[:len(ood_valid_dataset)]\n",
        "print('len(ood_valid_dataset): ', len(ood_valid_dataset), 'len(ood_invalid_dataset): ', len(ood_invalid_dataset))\n",
        "\n",
        "ood_dataset = ood_valid_dataset + ood_invalid_dataset\n",
        "print('len(train_dataset): ', len(train_dataset))\n",
        "print('len(test_dataset): ',  len(test_dataset))\n",
        "print('len(ood_dataset): ',  len(ood_dataset))\n",
        "\n",
        "dir = 'counting_dataset'\n",
        "\n",
        "\n",
        "# Write to file\n",
        "with open(dir + \"/train_dataset.txt\", \"w\") as f:\n",
        "    for data, label in train_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(dir + \"/test_dataset.txt\", \"w\") as f:\n",
        "    for data, label in test_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(dir + \"/ood_dataset.txt\", \"w\") as f:\n",
        "    for data, label in ood_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a medium complex dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsOoab2nQHSw",
        "outputId": "ca939605-f40f-4982-f7fd-2831f09ef9c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100000 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:00<00:00, 127572.41it/s]\n",
            " 82%|████████▏ | 81732/100000 [01:08<00:15, 1188.48it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_samples)):\n\u001b[1;32m     71\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         x \u001b[39m=\u001b[39m generate_invalid_string()\n\u001b[1;32m     73\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mlen\u001b[39m(x) \u001b[39m==\u001b[39m MAX_LENGTH):\n\u001b[1;32m     74\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[25], line 38\u001b[0m, in \u001b[0;36mgenerate_invalid_string\u001b[0;34m(min_length, max_length)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m START_TOKEN \u001b[39m+\u001b[39m PADDING_TOKEN \u001b[39m*\u001b[39m num_p \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mba\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m END_TOKEN \u001b[39m+\u001b[39m PADDING_TOKEN \u001b[39m*\u001b[39m (max_length \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m num_p)\n\u001b[1;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[39m# Random string of a's and b's which isn't a valid a*b* string\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     invalid_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(random\u001b[39m.\u001b[39;49mchoices(MAIN_CHARACTERS, k\u001b[39m=\u001b[39;49mlength))\n\u001b[1;32m     39\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mba\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m invalid_str:\n\u001b[1;32m     40\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/orion/u/yrichard/miniconda3/envs/tasksolver/lib/python3.10/random.py:519\u001b[0m, in \u001b[0;36mRandom.choices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    517\u001b[0m     floor \u001b[39m=\u001b[39m _floor\n\u001b[1;32m    518\u001b[0m     n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m    \u001b[39m# convert to float for a small speed improvement\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mreturn\u001b[39;00m [population[floor(random() \u001b[39m*\u001b[39m n)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _repeat(\u001b[39mNone\u001b[39;00m, k)]\n\u001b[1;32m    520\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cum_weights \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_accumulate(weights))\n",
            "File \u001b[0;32m/orion/u/yrichard/miniconda3/envs/tasksolver/lib/python3.10/random.py:519\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    517\u001b[0m     floor \u001b[39m=\u001b[39m _floor\n\u001b[1;32m    518\u001b[0m     n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m    \u001b[39m# convert to float for a small speed improvement\u001b[39;00m\n\u001b[0;32m--> 519\u001b[0m     \u001b[39mreturn\u001b[39;00m [population[floor(random() \u001b[39m*\u001b[39;49m n)] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m _repeat(\u001b[39mNone\u001b[39;00m, k)]\n\u001b[1;32m    520\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m     cum_weights \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(_accumulate(weights))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "    while True:\n",
        "        # Random string of a's and b's which isn't a valid a*b* string\n",
        "        invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "        if \"ba\" in invalid_str:\n",
        "            break\n",
        "    num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + invalid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "    )\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 100000  # Total number of samples\n",
        "\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in tqdm(range(num_samples)):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "print(len(valid_dataset))\n",
        "print(len(invalid_dataset))\n",
        "\n",
        "split = len(valid_dataset) * 4 // 5;\n",
        "train_dataset = valid_dataset[:split] + invalid_dataset[:split]\n",
        "test_dataset = valid_dataset[split:] + invalid_dataset[split:]\n",
        "\n",
        "num_ood_samples = 10000\n",
        "dataset = []\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "ood_valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "ood_invalid_dataset = list(set(dataset))[:len(ood_valid_dataset)]\n",
        "print(len(ood_valid_dataset), len(ood_invalid_dataset))\n",
        "\n",
        "ood_dataset = ood_valid_dataset + ood_invalid_dataset\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(ood_dataset))\n",
        "\n",
        "dir = 'counting_dataset'\n",
        "\n",
        "\n",
        "# Write to file\n",
        "with open(dir + \"/train_dataset.txt\", \"w\") as f:\n",
        "    for data, label in train_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(dir +\"/test_dataset.txt\", \"w\") as f:\n",
        "    for data, label in test_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"simple_dataset/ood_dataset.txt\", \"w\") as f:\n",
        "    for data, label in ood_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um9d6gaFQHS2",
        "outputId": "25950c37-ca96-44d8-eb16-a7424a5033bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 0.07164396345615387\n",
            "Accuracy: 0.9886124663526245\n",
            "Epoch 2/5, Loss: 0.03961213678121567\n",
            "Accuracy: 0.9935228802153432\n",
            "Epoch 3/5, Loss: 0.01588316820561886\n",
            "Accuracy: 0.9962882738896366\n",
            "Epoch 4/5, Loss: 0.0081285759806633\n",
            "Accuracy: 0.9956468707940781\n",
            "Epoch 5/5, Loss: 0.014825839549303055\n",
            "Accuracy: 0.996687836473755\n",
            "Epoch 1/5, Loss: 0.0060550919733941555\n",
            "Accuracy: 0.9971925471063257\n",
            "Epoch 2/5, Loss: 0.012896250933408737\n",
            "Accuracy: 0.9967614401076716\n",
            "Epoch 3/5, Loss: 0.014983288012444973\n",
            "Accuracy: 0.9972135767160162\n",
            "Epoch 4/5, Loss: 0.027989424765110016\n",
            "Accuracy: 0.9968140141318977\n",
            "Epoch 5/5, Loss: 0.007835699245333672\n",
            "Accuracy: 0.9973607839838493\n",
            "Accuracy: 0.9968455585464334\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants for model\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 2\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = StringDataset(\"train_dataset.txt\")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"shortrandloc200_400total_transformer_model.pth\")\n",
        "\n",
        "test_dataset = StringDataset(\"test_dataset.txt\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in test_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hYImCBy95ma"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgtAxiKqojlD",
        "outputId": "2465ce7b-98cf-4afa-b671-64dfe180f659"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9812460267005721\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQF3oa6oQHS5",
        "outputId": "32be32db-2898-4acf-8a3f-a3e4f7853756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 examples which are wrongly classified\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "spaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sbaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "sppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaaaaaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "spaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "spaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "spppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "spabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sppbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n"
          ]
        }
      ],
      "source": [
        "# Print 20 examples which are wrongly classified\n",
        "print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        if count == 20:\n",
        "            break\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            string = \"\".join([VALID_CHARACTERS[int(idx)] for idx in inputs[i]])\n",
        "            print(string, labels[i].item(), predicted[i].item())\n",
        "            count += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
