{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsOoab2nQHSw",
        "outputId": "b9aa7d3b-2f90-4abd-93ac-9253239131cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29855\n",
            "29855\n",
            "7471 7471\n",
            "47768\n",
            "11942\n",
            "14942\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define constants\n",
        "MAX_LENGTH = 250\n",
        "OOD_MAX_LENGTH = 500\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "num_p = 0\n",
        "\n",
        "# Function to generate valid a*b* strings\n",
        "def generate_valid_string(min_length = 0, max_length = MAX_LENGTH):\n",
        "    num_a = random.randint(0, max_length - 2)\n",
        "    num_b = random.randint(min(0, min_length - num_a), max_length - 2 - num_a)\n",
        "    valid_str = \"a\" * num_a + \"b\" * num_b\n",
        "    # num_p = random.randint(0, max_length - len(valid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + valid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * ((max_length - len(valid_str) - 2) - num_p)\n",
        "    )\n",
        "\n",
        "# Function to generate invalid strings\n",
        "def generate_invalid_string(min_length = 1, max_length = MAX_LENGTH):\n",
        "    length = random.randint(min_length, max_length - 2)\n",
        "    if length == 1:\n",
        "        # num_p = random.randint(0, max_length - 2)\n",
        "        return START_TOKEN + PADDING_TOKEN * num_p + \"ba\" + END_TOKEN + PADDING_TOKEN * (max_length - 2 - num_p)\n",
        "    while True:\n",
        "        # Random string of a's and b's which isn't a valid a*b* string\n",
        "        invalid_str = \"\".join(random.choices(MAIN_CHARACTERS, k=length))\n",
        "        if \"ba\" in invalid_str:\n",
        "            break\n",
        "    # num_p = random.randint(0, max_length - len(invalid_str) - 2)\n",
        "    return (\n",
        "        START_TOKEN\n",
        "        + PADDING_TOKEN * num_p\n",
        "        + invalid_str\n",
        "        + END_TOKEN\n",
        "        + PADDING_TOKEN * (max_length - len(invalid_str) - 2)\n",
        "    )\n",
        "\n",
        "\n",
        "# Generate dataset\n",
        "dataset = []\n",
        "num_samples = 200000  # Total number of samples\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string()\n",
        "        if not (len(x) == MAX_LENGTH):\n",
        "            continue\n",
        "        x += PADDING_TOKEN * (OOD_MAX_LENGTH - MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "invalid_dataset = list(set(dataset))[:len(valid_dataset)]\n",
        "print(len(valid_dataset))\n",
        "print(len(invalid_dataset))\n",
        "\n",
        "split = len(valid_dataset) * 4 // 5;\n",
        "train_dataset = valid_dataset[:split] + invalid_dataset[:split]\n",
        "test_dataset = valid_dataset[split:] + invalid_dataset[split:]\n",
        "\n",
        "num_ood_samples = 10000\n",
        "dataset = []\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_valid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 1))\n",
        "        break\n",
        "\n",
        "# remove duplicates\n",
        "ood_valid_dataset = list(set(dataset))\n",
        "dataset = []\n",
        "\n",
        "for _ in range(num_ood_samples):\n",
        "    while True:\n",
        "        x = generate_invalid_string(min_length=MAX_LENGTH + 2, max_length=OOD_MAX_LENGTH)\n",
        "        if not (len(x) == OOD_MAX_LENGTH):\n",
        "            continue\n",
        "        dataset.append((x, 0))\n",
        "        break\n",
        "\n",
        "# Remove all duplicates\n",
        "ood_invalid_dataset = list(set(dataset))[:len(ood_valid_dataset)]\n",
        "print(len(ood_valid_dataset), len(ood_invalid_dataset))\n",
        "\n",
        "ood_dataset = ood_valid_dataset + ood_invalid_dataset\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(ood_dataset))\n",
        "\n",
        "\n",
        "# Write to file\n",
        "with open(\"train_dataset_nopadded.txt\", \"w\") as f:\n",
        "    for data, label in train_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"test_dataset_nopadded.txt\", \"w\") as f:\n",
        "    for data, label in test_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")\n",
        "\n",
        "with open(\"ood_dataset_nopadded.txt\", \"w\") as f:\n",
        "    for data, label in ood_dataset:\n",
        "        f.write(f\"{data} {label}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Um9d6gaFQHS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "126233aa-295d-411d-dac3-4580d4e354fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.38084810972213745\n",
            "Accuracy: 0.9324652487020599\n",
            "Epoch 2/5, Loss: 0.2495858073234558\n",
            "Accuracy: 0.9465960475632222\n",
            "Epoch 3/5, Loss: 0.18593522906303406\n",
            "Accuracy: 0.955870038519511\n",
            "Epoch 4/5, Loss: 0.2120962142944336\n",
            "Accuracy: 0.9533369619829174\n",
            "Epoch 5/5, Loss: 0.27028077840805054\n",
            "Accuracy: 0.9480405292245855\n",
            "Epoch 1/5, Loss: 0.15211986005306244\n",
            "Accuracy: 0.9499665047730699\n",
            "Epoch 2/5, Loss: 0.13904444873332977\n",
            "Accuracy: 0.9487104337631888\n",
            "Epoch 3/5, Loss: 0.08156761527061462\n",
            "Accuracy: 0.9491291240998158\n",
            "Epoch 4/5, Loss: 0.18180182576179504\n",
            "Accuracy: 0.9518087422542287\n",
            "Epoch 5/5, Loss: 0.22958149015903473\n",
            "Accuracy: 0.950217718975046\n",
            "Accuracy: 0.9450678278345336\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define constants for model\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "\n",
        "# Transformer model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "# Prepare dataset and dataloader\n",
        "dataset = StringDataset(\"train_dataset_nopadded.txt\")\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    for inputs, labels in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.to(device))\n",
        "        loss = criterion(outputs.squeeze(), labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}\")\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in dataloader:\n",
        "        outputs = model(inputs.to(device))\n",
        "        predicted = torch.round(outputs)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "    print(f\"Accuracy: {correct/total}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"1head_1layer_embed6batch512hidden1_100max200ood0pad_total_transformer_model.pth\")\n",
        "\n",
        "test_dataset = StringDataset(\"test_dataset_nopadded.txt\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in test_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgtAxiKqojlD",
        "outputId": "b050b55a-9bd5-422c-cebb-7a189e79895a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6351893990095034\n"
          ]
        }
      ],
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_nopadded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQF3oa6oQHS5",
        "outputId": "ef5f5146-b590-4bc3-f1a5-57de88f25be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20 examples which are wrongly classified\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababbaaaaaaaaaabbbbaaaabaababaabaabbbbaabaabbbbabaaaabbaababbaaaabbabbbabaabbababbababbabbaaaaabbabaaabbaaaaaabaaabbbaabbaaabbaaaabaababaaaabbaaabbbbabaabbabaaaabbbababaaaaabbabaababbbaaaabbaaabbbabaaabbbbabbbaaaababbbabbbaababaabbbabaaabbaabbbbaaababbaaabaabbbbaaabbabaaaaaabaaabaabbbbbaaaaaabaababbbbbabaaabbbabbbabbbbbbbabbbaabbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaabbbbaababaabababbbbbbbbbbabaabbbabbbaabbaabbbaabbbabbabbabbbbaaabbababbaabbbabbaaabbabbbbabbbbabbaaabaabaaaabaabbababaaababababbbbaaababaaaaabaabbbaaabbabaababbababaabaaabbaaabbbbaabbbbbbbbababaaaabaabaabaaababbbabbbbabaabaabbbbaaaabaabaaaabbbbabaaaabbbabbbbbababbabbabaababbbabaaaabbbbaabbbaabaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaabbabaaaaabaababbabbaabbabbbbaabbabbaaabbababbbbbbbabaabbbaaaaaababaaaaaaaabbabbbababbaabbabaabbaaaaabaabbabbaaaaaaabbabbabbbaaabbabbbbabbabbbbaabbbbaaaaaabaaaaaabbbabaaaabaaaababaaabepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaabbaaaaabbabbbbaababaaaababbabbbaaaaabbababbababaaabbaababbbbabbaaabaabbbaabbbbbaaabababbabbbaaabbbabbaabbaaaabbabbabaabbbabbbbbabaaaaaabbaabbaababaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbabaaabbaaababaabbaaabaaabbaababbabbbbbaabaabbbbaabbaabaabbbbbabaaaabbabaabaabaabbaaabababbbabaaabaaaababbbabaabaaaaabaaaaaaabbaaaaaaabababbbbbabaabbabbbababbbbbbaaabaaaaabaaababbaaaaaababababaabbabbbbbbbabbbababbbaabbbbbabbaabbbbbabbbbbaabababbbbbbbbaabaabbaaaabbbabaabaabaabbbbbaabbbbbbbabbbabababbaaaababbbbbabababaabbbaaaaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "sbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaaaaabbabaababaaaababbbabbbbaaabbbbaaababababbababbbaabbbabaabbababbabbbbbababbababbabbbaabbabbaabbbbabbbaaabbbbabbabaabbaababaabaabaabbabaaaabbabbbaaaabaaaabaaabaabaaaaababbbabaaababbabababbbbaeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 1.0 0.0\n",
            "saaaaababaaabbaabaabbbaaababbaaaabbbbaabbbbaaababbaabaabaaaabaabbaaababaabbbbabbaababbaabaababaaabbabbbbbaaabaaepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaabaabaaabbbbabbbabaaabababbaaaaaababbabaabbabbbbbbbababababaaabaababaabbaaaabaabbabbbabbbbabbbaaaaabababbbbabbabbbaaabbaabeppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaababbaaabbbabbbaabaaababaaabaabbaaababbaaaabbaabbabbabbaabbabaabaaaaaababbababbbababbabbabbbaaaaabbbbbbababaaababbbabbaaaabbbbabbabaaaaaabbbbabbbaaabbaaaabbaaabaabbabbbabaaaaabbbabbababaaabbaaaabbbabaabaaaaabbaababbbbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaabaaaababbaaaaabaaaababbababbbbababaabaabaabbbbbbaaaaabbaabbabaababbaabbbbbbbbbabbbaabaaabbabaabbabbaababbbaabbbbaababbbaaabbbbbaaabbbbbaaaabbbbbbbaaabbbabbbaabbaaabbbababbbaababababaaabbabaaabaabbaababaaabbaabababaabababaaaabaabbbabaaaabaaabbaaaabababbaaabbaabaaaababababaabababbaaababaaabaabbababbabbbbbbbbbaaaaabaabbbabbbaaaabbabbbbbabaabbbababaabaabababbbabbaaaaaabaabbbabaaabbaabbaabaababbbabaaaabaabaababbbbababbbbabaaabaaabaababbbabbaabbbabaaaabbaaababbbabbaabaababbabababbbabaabbaabbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaabbaabaabbabbbbaababbbaaaaababbbaababbaabbaababaababaaabababbaaabababababbbbbbbaaababbaaabaabbaabaaabaabbbbaababbbbaaaaaaaabaabbaaabbabaababbbabababaabbbaabbaaabbabaaabbababaaaabbbaabaabbbbabbbbaaabbbbaabbababbbababbababababaabaaaaaaaabbabaababababbbbababaababaaaabaaaabababbbabbbabbbaaabababbabbabbabaaaabbbabaaababbbaaaaaabaaabaaaaababababbabaababbbaaaabaaaaabbabaababaaaaababaabaaabaabababaababbaababbabbbaaabbbabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n",
            "saaaaaaabbbabbabbbbbbaaabaaaaabbabaaabaabbaaaabaabbbaabaabbbbbbbbabaabbbbbaabbbaabbaabbaabbbbababaaabbaaabbbbbaabbaabbababaaabbaaabbaaaabbaababbaaaaababaabaaababaabbbbbaabaabbabbbbababbbbbaababbaababbbbbaabbababbaaaaaabbbababaabaaaaaaabbaabababaaabbaababbbbbbaabbabbbaabababbbbbbaabbepppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppppp 0.0 1.0\n"
          ]
        }
      ],
      "source": [
        "# Print 20 examples which are wrongly classified\n",
        "print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        if count == 20:\n",
        "            break\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            string = \"\".join([VALID_CHARACTERS[int(idx)] for idx in inputs[i]])\n",
        "            print(string, labels[i].item(), predicted[i].item())\n",
        "            count += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model_2 = TransformerClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_2.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_100max500ood_total_transformer_model.pth'))\n",
        "model_2.to(device)\n",
        "\n",
        "# Print 20 examples which are wrongly classified\n",
        "# print(\"20 examples which are wrongly classified\")\n",
        "count = 0\n",
        "matrix = np.zeros((2, 2))\n",
        "num_pad_zeros = np.zeros((500))\n",
        "for inputs, labels in dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    for i in range(len(predicted)):\n",
        "        matrix[int(predicted[i]), int(labels[i])] += 1\n",
        "        if predicted[i] != labels[i]:\n",
        "            # Convert back to string of a's and b's\n",
        "            num_start_pad = 0\n",
        "            for j in range(len(inputs[i])):\n",
        "                if VALID_CHARACTERS[int(inputs[i][j])] != 'p':\n",
        "                    break\n",
        "                num_start_pad += 1\n",
        "            num_pad_zeros[num_start_pad] += 1\n",
        "print(matrix)\n",
        "print(num_pad_zeros)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0WOiMFYFLKF",
        "outputId": "83eb16a1-f999-4d3b-9c98-eab2c0e43b37"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.3878e+04 1.2457e+04]\n",
            " [6.0000e+00 1.1427e+04]]\n",
            "[12463.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ood_dataset = StringDataset(\"ood_dataset_gappadded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# Print accuracy of the model\n",
        "model_2.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_2(inputs.to(device))\n",
        "    predicted = torch.round(outputs)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.squeeze().to(device) == labels.to(device)).sum().item()\n",
        "print(f\"Accuracy: {correct/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ekqsmo6JWde",
        "outputId": "6057ec8a-00aa-4f05-fe39-bcdef45b8e2a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9748629848229342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_2.transformer_encoder.layers[0].self_attn.in_proj_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtNf76gYGIIe",
        "outputId": "3ae629bc-80b3-40f3-c7f7-6ddc0f6f81f6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.1173, -0.4868, -0.2787, -0.3836,  0.4869,  0.4462],\n",
            "        [-0.3077,  0.1080, -0.0703,  0.2785,  0.0020,  0.3271],\n",
            "        [-0.3017, -0.1836, -0.0864, -0.2321, -0.2330, -0.4434],\n",
            "        [-0.0861,  0.6321, -0.4590, -0.3281, -0.5029, -0.4216],\n",
            "        [ 0.4099, -0.1211, -0.1174, -0.1267, -0.3700, -0.1985],\n",
            "        [ 0.3000,  0.4385, -0.3269,  0.3167,  0.4170, -0.3065],\n",
            "        [ 0.1610, -0.0720,  0.3529,  0.4052,  0.0425,  0.2735],\n",
            "        [-0.3191, -0.0640,  0.2417, -0.0950,  0.0050, -0.3506],\n",
            "        [-0.3651, -0.1423, -0.1854, -0.2698,  0.1397,  0.0741],\n",
            "        [ 0.4333,  0.2350, -0.0254,  0.4441,  0.2188,  0.1552],\n",
            "        [ 0.1111, -0.1373, -0.0707,  0.4105,  0.1355,  0.3309],\n",
            "        [-0.2359, -0.2420, -0.0904,  0.2508,  0.3944, -0.0200],\n",
            "        [-0.2139,  0.3799, -0.1082, -0.3886,  0.0216, -0.0505],\n",
            "        [-0.2125,  0.3423, -0.3185,  0.3012, -0.3671,  0.2281],\n",
            "        [-0.4219,  0.1814,  0.2821,  0.0127,  0.1133,  0.3603],\n",
            "        [-0.2622, -0.2055,  0.4117,  0.4700,  0.1935,  0.2158],\n",
            "        [ 0.0299, -0.0371, -0.0226,  0.0525,  0.0328, -0.4088],\n",
            "        [-0.1115,  0.3936, -0.3476,  0.4100, -0.1752,  0.3781]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "VALID_CHARACTERS = [\"s\", \"a\", \"b\", \"e\", \"p\"]\n",
        "MAIN_CHARACTERS = [\"a\", \"b\"]\n",
        "START_TOKEN = \"s\"\n",
        "END_TOKEN = \"e\"\n",
        "PADDING_TOKEN = \"p\"\n",
        "VALID_RATIO = 0.5  # Half of the dataset should be valid a*b* strings\n",
        "VOCAB_SIZE = len(VALID_CHARACTERS)\n",
        "EMBEDDING_DIM = 6\n",
        "NUM_HEADS = 2\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_DIM = 1\n",
        "BATCH_SIZE = 512\n",
        "EPOCHS = 5\n",
        "MAX_LENGTH = 200\n",
        "OOD_MAX_LENGTH = 400\n",
        "\n",
        "class TransformerEncoderLayerWithAttention(nn.TransformerEncoderLayer):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation='relu'):\n",
        "        super(TransformerEncoderLayerWithAttention, self).__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src,\n",
        "            src_mask = None,\n",
        "            src_key_padding_mask = None,\n",
        "            is_causal: bool = False):\n",
        "        src_key_padding_mask = F._canonical_mask(\n",
        "            mask=src_key_padding_mask,\n",
        "            mask_name=\"src_key_padding_mask\",\n",
        "            other_type=F._none_or_dtype(src_mask),\n",
        "            other_name=\"src_mask\",\n",
        "            target_type=src.dtype\n",
        "        )\n",
        "\n",
        "        src_mask = F._canonical_mask(\n",
        "            mask=src_mask,\n",
        "            mask_name=\"src_mask\",\n",
        "            other_type=None,\n",
        "            other_name=\"\",\n",
        "            target_type=src.dtype,\n",
        "            check_other=False,\n",
        "        )\n",
        "\n",
        "        is_fastpath_enabled = torch.backends.mha.get_fastpath_enabled()\n",
        "\n",
        "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
        "        why_not_sparsity_fast_path = ''\n",
        "        if not is_fastpath_enabled:\n",
        "            why_not_sparsity_fast_path = \"torch.backends.mha.get_fastpath_enabled() was not True\"\n",
        "        elif not src.dim() == 3:\n",
        "            why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
        "        elif self.training:\n",
        "            why_not_sparsity_fast_path = \"training is enabled\"\n",
        "        elif not self.self_attn.batch_first:\n",
        "            why_not_sparsity_fast_path = \"self_attn.batch_first was not True\"\n",
        "        elif self.self_attn.in_proj_bias is None:\n",
        "            why_not_sparsity_fast_path = \"self_attn was passed bias=False\"\n",
        "        elif not self.self_attn._qkv_same_embed_dim:\n",
        "            why_not_sparsity_fast_path = \"self_attn._qkv_same_embed_dim was not True\"\n",
        "        elif not self.activation_relu_or_gelu:\n",
        "            why_not_sparsity_fast_path = \"activation_relu_or_gelu was not True\"\n",
        "        elif not (self.norm1.eps == self.norm2.eps):\n",
        "            why_not_sparsity_fast_path = \"norm1.eps is not equal to norm2.eps\"\n",
        "        elif src.is_nested and (src_key_padding_mask is not None or src_mask is not None):\n",
        "            why_not_sparsity_fast_path = \"neither src_key_padding_mask nor src_mask are not supported with NestedTensor input\"\n",
        "        elif self.self_attn.num_heads % 2 == 1:\n",
        "            why_not_sparsity_fast_path = \"num_head is odd\"\n",
        "        elif torch.is_autocast_enabled():\n",
        "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
        "        if not why_not_sparsity_fast_path:\n",
        "            tensor_args = (\n",
        "                src,\n",
        "                self.self_attn.in_proj_weight,\n",
        "                self.self_attn.in_proj_bias,\n",
        "                self.self_attn.out_proj.weight,\n",
        "                self.self_attn.out_proj.bias,\n",
        "                self.norm1.weight,\n",
        "                self.norm1.bias,\n",
        "                self.norm2.weight,\n",
        "                self.norm2.bias,\n",
        "                self.linear1.weight,\n",
        "                self.linear1.bias,\n",
        "                self.linear2.weight,\n",
        "                self.linear2.bias,\n",
        "            )\n",
        "\n",
        "            # We have to use list comprehensions below because TorchScript does not support\n",
        "            # generator expressions.\n",
        "            _supported_device_type = [\"cpu\", \"cuda\", torch.utils.backend_registration._privateuse1_backend_name]\n",
        "            if torch.overrides.has_torch_function(tensor_args):\n",
        "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
        "            elif not all((x.device.type in _supported_device_type) for x in tensor_args):\n",
        "                why_not_sparsity_fast_path = (\"some Tensor argument's device is neither one of \"\n",
        "                                              f\"{_supported_device_type}\")\n",
        "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
        "                why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
        "                                              \"input/output projection weights or biases requires_grad\")\n",
        "\n",
        "            if not why_not_sparsity_fast_path:\n",
        "                merged_mask, mask_type = self.self_attn.merge_masks(src_mask, src_key_padding_mask, src)\n",
        "                return torch._transformer_encoder_layer_fwd(\n",
        "                    src,\n",
        "                    self.self_attn.embed_dim,\n",
        "                    self.self_attn.num_heads,\n",
        "                    self.self_attn.in_proj_weight,\n",
        "                    self.self_attn.in_proj_bias,\n",
        "                    self.self_attn.out_proj.weight,\n",
        "                    self.self_attn.out_proj.bias,\n",
        "                    self.activation_relu_or_gelu == 2,\n",
        "                    self.norm_first,\n",
        "                    self.norm1.eps,\n",
        "                    self.norm1.weight,\n",
        "                    self.norm1.bias,\n",
        "                    self.norm2.weight,\n",
        "                    self.norm2.bias,\n",
        "                    self.linear1.weight,\n",
        "                    self.linear1.bias,\n",
        "                    self.linear2.weight,\n",
        "                    self.linear2.bias,\n",
        "                    merged_mask,\n",
        "                    mask_type,\n",
        "                )\n",
        "\n",
        "\n",
        "        x = src\n",
        "        if self.norm_first:\n",
        "            x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask, is_causal=is_causal)\n",
        "            x = x + self._ff_block(self.norm2(x))\n",
        "        else:\n",
        "            x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal))\n",
        "            x = self.norm2(x + self._ff_block(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderWithAttention(nn.TransformerEncoder):\n",
        "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
        "        super(TransformerEncoderWithAttention, self).__init__(encoder_layer, num_layers, norm)\n",
        "\n",
        "    def forward(self, src, mask=None, src_key_padding_mask=None):\n",
        "        output = src\n",
        "\n",
        "        attentions = []  # List to store attention scores from each layer\n",
        "\n",
        "        for mod in self.layers:\n",
        "            output, attention_scores = mod(output)\n",
        "            attentions.append(attention_scores)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            output = self.norm(output)\n",
        "\n",
        "        return output, attentions\n",
        "\n",
        "# Transformer model\n",
        "class TransformerDebugClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
        "        super(TransformerDebugClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pos_encoder = nn.Parameter(torch.zeros(1, OOD_MAX_LENGTH, embedding_dim))\n",
        "        encoder_layer = TransformerEncoderLayerWithAttention(\n",
        "            embedding_dim, num_heads, hidden_dim\n",
        "        )\n",
        "        self.transformer_encoder = TransformerEncoderWithAttention(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(OOD_MAX_LENGTH * embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        x = self.embedding(x) + self.pos_encoder\n",
        "        print(x.shape)\n",
        "        x, attn = self.transformer_encoder(x)\n",
        "        print(x.shape)\n",
        "        print(attn)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return self.sigmoid(x)"
      ],
      "metadata": {
        "id": "K2F50v9OH4EL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Mapping characters to indices\n",
        "char_to_index = {ch: idx for idx, ch in enumerate(VALID_CHARACTERS)}\n",
        "\n",
        "# Custom dataset class\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split(\" \")\n",
        "                self.data.append(parts[0])\n",
        "                self.labels.append(int(parts[1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        string = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoded = self.encode_string(string)\n",
        "        return torch.tensor(encoded, dtype=torch.long), torch.tensor(\n",
        "            label, dtype=torch.float32\n",
        "        )\n",
        "\n",
        "    def encode_string(self, string):\n",
        "        return [char_to_index[char] for char in string]\n",
        "\n",
        "model_3 = TransformerDebugClassifier(\n",
        "    VOCAB_SIZE, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS\n",
        ")\n",
        "\n",
        "model_3.load_state_dict(torch.load('1head_1layer_embed6batch512hidden1_200max400ood_total_transformer_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "ood_dataset = StringDataset(\"ood_dataset_padded.txt\")\n",
        "ood_dataloader = DataLoader(ood_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "for inputs, labels in ood_dataloader:\n",
        "    outputs = model_3(inputs)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "qvrxJ8MMIZOV",
        "outputId": "d15d0bc0-008c-4bfd-a83b-3b3e1e6abf8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 400])\n",
            "torch.Size([512, 400, 6])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MultiheadAttention' object has no attribute 'data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-236764849ed1>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mood_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mood_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mood_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4d4344d7ed5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4d4344d7ed5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mattentions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-4d4344d7ed5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Get attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mattention_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming self_attn is a list of MultiheadAttention layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MultiheadAttention' object has no attribute 'data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KuNUMejaFKkY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}